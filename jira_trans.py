import os
import re
from collections.abc import Sequence
from dataclasses import dataclass
from pathlib import Path
from typing import Optional
from urllib.parse import urlparse
from dotenv import load_dotenv

import json
import base64
import urllib.parse

import requests
from openai import OpenAI
from pydantic import BaseModel


@dataclass
class TranslationChunk:
    id: str
    field: str
    original_text: str
    clean_text: str
    attachments: list[str]
    header: Optional[str] = None


class TranslationItem(BaseModel):
    id: str
    translated: str


class TranslationResponse(BaseModel):
    translations: list[TranslationItem]


@dataclass
class FieldTranslationJob:
    field: str
    original_value: str
    chunks: list[TranslationChunk]
    mode: str = "default"


class JiraTicketTranslator:
    """Jira í‹°ì¼“ì„ ë²ˆì—­í•˜ë©´ì„œ ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ ë§ˆí¬ì—…ì„ ìœ ì§€í•˜ëŠ” í´ë˜ìŠ¤"""

    # ì„¹ì…˜ í—¤ë”ëŠ” "ì˜ì–´ ë¶€ë¶„" ê¸°ì¤€ìœ¼ë¡œ ê´€ë¦¬
    # ì˜ˆì‹œ:
    #   - "Observed:"
    #   - "Observed/ê´€ì°°ë¨:"
    #   - "Expected/ê¸°ëŒ€ ê²°ê³¼:"
    #   - "Expected Result/ê¸°ëŒ€ ê²°ê³¼:"
    #   - "Note/ì°¸ê³ :"
    #   - "Video/ì˜ìƒ:"
    # ì˜ì–´-only í—¤ë”ì™€ ì˜ì–´/êµ­ë¬¸ í˜¼í•© í—¤ë”ë¥¼ ëª¨ë‘ í¬ì°©í•˜ê¸° ìœ„í•´
    # ê°€ëŠ¥í•œ ì˜ì–´ í˜•íƒœë“¤ì„ ë‚˜ì—´í•´ ë‘”ë‹¤.
    DESCRIPTION_SECTIONS = ("Observed", "Expected", "Expected Result", "Note", "Notes", "Video", "Etc.")

    def __init__(self, jira_url: str, email: str, api_token: str, openai_api_key: str):
        """
        Args:
            jira_url: Jira ì¸ìŠ¤í„´ìŠ¤ URL (ì˜ˆ: 'https://cloud.jira.krafton.com')
            email: Jira ê³„ì • ì´ë©”ì¼
            api_token: Jira API í† í°
            openai_api_key: OpenAI API í‚¤
        """
        self.jira_url = jira_url.rstrip("/")
        self.email = email
        self.api_token = api_token

        self.session = requests.Session()
        self.session.auth = (email, api_token)

        # OpenAI SDK ì´ˆê¸°í™” (LangChain ëŒ€ì²´)
        self.openai = OpenAI(api_key=openai_api_key)
        self.openai_model = os.getenv("OPENAI_MODEL", "gpt-5.1")
        
        # ìš©ì–´ì§‘ ë°ì´í„° (translate_issue í˜¸ì¶œ ì‹œ ë¡œë“œë¨)
        self.glossary_terms: dict[str, str] = {}
        self.glossary_name: str = ""

    def extract_attachments_markup(self, text: str) -> tuple[list[str], str]:
        """
        Jira ë§ˆí¬ì—…ì—ì„œ ì´ë¯¸ì§€ì™€ ì²¨ë¶€íŒŒì¼ ë§ˆí¬ì—…ì„ ì¶”ì¶œí•˜ê³  í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ëŒ€ì²´

        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸

        Returns:
            (ë§ˆí¬ì—… ë¦¬ìŠ¤íŠ¸, í”Œë ˆì´ìŠ¤í™€ë”ê°€ ì ìš©ëœ í…ìŠ¤íŠ¸)
        """
        if not text:
            return [], ""

        attachments = []

        # ì´ë¯¸ì§€ ë§ˆí¬ì—… íŒ¨í„´: !image.png!, !image.png|thumbnail!, !image.png|width=300!
        image_pattern = r'!([^!]+?)(?:\|[^!]*)?!'

        # ì²¨ë¶€íŒŒì¼ ë§ˆí¬ì—… íŒ¨í„´: [^attachment.pdf], [^video.mp4]
        attachment_pattern = r'\[\^([^\]]+?)\]'

        def replace_image(match):
            attachments.append(match.group(0))
            return f"__IMAGE_PLACEHOLDER_{len(attachments)-1}__"

        def replace_attachment(match):
            attachments.append(match.group(0))
            return f"__ATTACHMENT_PLACEHOLDER_{len(attachments)-1}__"

        # í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ëŒ€ì²´
        text = re.sub(image_pattern, replace_image, text)
        text = re.sub(attachment_pattern, replace_attachment, text)

        return attachments, text

    def restore_attachments_markup(self, text: str, attachments: list[str]) -> str:
        """
        ë²ˆì—­ëœ í…ìŠ¤íŠ¸ì— ì›ë³¸ ë§ˆí¬ì—…ì„ ë³µì›

        Args:
            text: ë²ˆì—­ëœ í…ìŠ¤íŠ¸ (í”Œë ˆì´ìŠ¤í™€ë” í¬í•¨)
            attachments: ì›ë³¸ ë§ˆí¬ì—… ë¦¬ìŠ¤íŠ¸

        Returns:
            ë§ˆí¬ì—…ì´ ë³µì›ëœ í…ìŠ¤íŠ¸
        """
        for i, attachment_markup in enumerate(attachments):
            # ì´ë¯¸ì§€ í”Œë ˆì´ìŠ¤í™€ë” ë³µì›
            text = text.replace(f"__IMAGE_PLACEHOLDER_{i}__", attachment_markup)
            # ì²¨ë¶€íŒŒì¼ í”Œë ˆì´ìŠ¤í™€ë” ë³µì›
            text = text.replace(f"__ATTACHMENT_PLACEHOLDER_{i}__", attachment_markup)

        return text

    def translate_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­ (ë§ˆí¬ì—… ì œì™¸)
        í•œê¸€ í…ìŠ¤íŠ¸ëŠ” ì˜ì–´ë¡œ, ì˜ì–´ í…ìŠ¤íŠ¸ëŠ” í•œê¸€ë¡œ ìë™ ë²ˆì—­.

        Args:
            text: ë²ˆì—­í•  í…ìŠ¤íŠ¸

        Returns:
            ë²ˆì—­ëœ í…ìŠ¤íŠ¸
        """
        if not text or not text.strip():
            return text

        # ì–¸ì–´ ê°ì§€ ë¨¼ì € ìˆ˜í–‰
        detected_lang = self._detect_text_language(text)
        
        glossary_instruction = self._build_glossary_instruction([text])
        
        if detected_lang == "ko":
            # í•œêµ­ì–´ â†’ ì˜ì–´: ì™„ì „í•œ ì˜ì–´ë¡œ ë²ˆì—­
            system_msg = (
                "You are a professional Korean to English translator. "
                "Translate the following Korean text to English. "
                "The output MUST be 100% in English - do NOT leave any Korean words. "
                "Preserve Jira markup (*bold*, _italic_, {{code}}, etc.)."
            )
        else:
            # ì˜ì–´ â†’ í•œêµ­ì–´: ê³ ìœ ëª…ì‚¬ëŠ” ì˜ì–´ ìœ ì§€
            system_msg = (
                "You are a professional English to Korean translator. "
                "Translate the following English text to Korean. "
                "Keep proper nouns and game-specific terms in English. "
                "Use terse memo-style (ìŒìŠ´ì²´): drop endings such as 'í•©ë‹ˆë‹¤', "
                "favor noun phrases like 'í•˜ì´ë“œì•„ì›ƒ ì§„ì…', 'ì´ìŠˆ í™•ì¸'. "
                "Preserve Jira markup (*bold*, _italic_, {{code}}, etc.)."
            )
        
        if glossary_instruction:
            system_msg = f"{system_msg} {glossary_instruction}"

        response = self.openai.chat.completions.create(
            model=self.openai_model,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": text},
            ],
        )
        return (response.choices[0].message.content or "").strip()

    def _build_glossary_instruction(self, texts: Sequence[str]) -> str:
        """
        ì–‘ë°©í–¥ ìš©ì–´ì§‘ ì§€ì›: ì˜ì–´â†’í•œêµ­ì–´, í•œêµ­ì–´â†’ì˜ì–´ ëª¨ë‘ í¬í•¨.
        ë‹¨ì–´ ê²½ê³„ ë§¤ì¹­(\b)ì„ ì‚¬ìš©í•˜ì—¬ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ìš©ì–´ë§Œ ì°¾ìŠµë‹ˆë‹¤.
        ì˜ˆ: 'key' ê²€ìƒ‰ ì‹œ 'monkey'ëŠ” ë¬´ì‹œí•¨.
        """
        terms = self.glossary_terms
        if not terms:
            return ""

        combined_text = "\n".join(texts).lower()
        glossary_lines: list[str] = []

        for eng, kor in terms.items():
            # ì˜ì–´ ìš©ì–´ê°€ í…ìŠ¤íŠ¸ì— ìˆìœ¼ë©´ ì˜ì–´â†’í•œêµ­ì–´ ë§¤í•‘ ì¶”ê°€
            eng_pattern = r"\b" + re.escape(eng.lower()) + r"\b"
            if re.search(eng_pattern, combined_text):
                glossary_lines.append(f"- {eng} <-> {kor}")
                continue
            
            # í•œêµ­ì–´ ìš©ì–´ê°€ í…ìŠ¤íŠ¸ì— ìˆìœ¼ë©´ í•œêµ­ì–´â†’ì˜ì–´ ë§¤í•‘ ì¶”ê°€
            if kor.lower() in combined_text:
                glossary_lines.append(f"- {kor} <-> {eng}")

        if not glossary_lines:
            return ""

        glossary_name_display = self.glossary_name or "Project"
        return (
            f"Use this {glossary_name_display} glossary for specific terms "
            "(bidirectional mapping):\n"
            + "\n".join(glossary_lines)
        )

    def translate_field(self, field_value: str) -> str:
        """
        Jira í•„ë“œ ê°’ì„ ë²ˆì—­ (ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ ë§ˆí¬ì—… ë³´ì¡´)
        í•œê¸€â†’ì˜ì–´, ì˜ì–´â†’í•œê¸€ ìë™ ë²ˆì—­.

        Args:
            field_value: ì›ë³¸ í•„ë“œ ê°’

        Returns:
            ë²ˆì—­ëœ í•„ë“œ ê°’ (ë§ˆí¬ì—… ë³´ì¡´)
        """
        if not field_value:
            return field_value

        # 1. ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ ë§ˆí¬ì—… ì¶”ì¶œ
        attachments, clean_text = self.extract_attachments_markup(field_value)

        # 2. í…ìŠ¤íŠ¸ë§Œ ë²ˆì—­
        translated_text = self.translate_text(clean_text)

        # 3. ë§ˆí¬ì—… ë³µì›
        final_text = self.restore_attachments_markup(translated_text, attachments)

        return final_text

    def _translate_chunk_text(
        self,
        chunk: TranslationChunk,
    ) -> str:
        return self.translate_text(chunk.clean_text) or ""

    def _translate_chunk_list(
        self,
        chunk_list: Sequence[TranslationChunk],
    ) -> dict[str, str]:
        translations: dict[str, str] = {}
        for chunk in chunk_list:
            translations[chunk.id] = self._translate_chunk_text(chunk)
        return translations

    def _translate_chunks_individually(
        self,
        jobs: dict[str, FieldTranslationJob],
    ) -> dict[str, str]:
        per_chunk: dict[str, str] = {}
        for job in jobs.values():
            per_chunk.update(self._translate_chunk_list(job.chunks))
        return per_chunk

    def translate_description_field(self, field_value: str) -> str:
        """í•œê¸€â†’ì˜ì–´, ì˜ì–´â†’í•œê¸€ ìë™ ë²ˆì—­."""
        sections = self._extract_description_sections(field_value)

        if not sections:
            translated = self.translate_field(field_value)
            return self._format_bilingual_block(field_value, translated)

        formatted_sections = []
        for header, content in sections:
            translated_section = self.translate_field(content)
            formatted_sections.append(
                self._format_bilingual_block(content, translated_section, header=header)
            )

        return "\n\n".join(filter(None, formatted_sections)).strip()

    def _detect_text_language(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€ (ê³ ë„í™”ëœ ë¡œì§).
        
        í•µì‹¬ ì›ì¹™:
        - í•œêµ­ì–´ ì¡°ì‚¬/ì–´ë¯¸ê°€ ìˆìœ¼ë©´ ê±°ì˜ í™•ì‹¤íˆ í•œêµ­ì–´ (ì˜ì–´ ê³ ìœ ëª…ì‚¬ê°€ ë§ì•„ë„)
        - í•œê¸€ì´ 1ìë¼ë„ ìˆê³  ë¬¸ì¥ êµ¬ì¡°ê°€ í•œêµ­ì–´ë©´ í•œêµ­ì–´
        - ìˆœìˆ˜ ì˜ì–´ ë¬¸ì¥ íŒ¨í„´ì´ ìˆì„ ë•Œë§Œ ì˜ì–´ë¡œ íŒë‹¨
        
        Returns:
            "ko": í•œêµ­ì–´
            "en": ì˜ì–´
            "unknown": ì•Œ ìˆ˜ ì—†ìŒ
        """
        if not text:
            return "unknown"
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì–¸ì–´ ê°ì§€ (ë§ˆí¬ì—… ì œê±° ì „)
        original_text = text
        
        # ë§ˆí¬ì—… ì œê±°ëœ í…ìŠ¤íŠ¸
        sanitized = self._extract_detectable_text(text)
        if not sanitized:
            return "unknown"
        
        # 1. í•œê¸€ ë¬¸ì ê°œìˆ˜
        korean_chars = len(re.findall(r"[\uac00-\ud7a3]", sanitized))
        
        # 2. ì˜ì–´ ë¬¸ì ê°œìˆ˜
        latin_chars = len(re.findall(r"[A-Za-z]", sanitized))
        
        # 3. í•œêµ­ì–´ ì¡°ì‚¬ íŒ¨í„´ (ê°€ì¥ í™•ì‹¤í•œ í•œêµ­ì–´ ì§€í‘œ)
        # í•œê¸€ + ì¡°ì‚¬ íŒ¨í„´: ì˜ì–´ ê³ ìœ ëª…ì‚¬ ë’¤ì— í•œêµ­ì–´ ì¡°ì‚¬ê°€ ë¶™ëŠ” ê²½ìš°ë„ í¬í•¨
        korean_particle_patterns = [
            # ì£¼ê²©/ëª©ì ê²© ì¡°ì‚¬
            r'[\uac00-\ud7a3][ì´ê°€](?:\s|$|[^\uac00-\ud7a3])',  # ~ì´/ê°€
            r'[\uac00-\ud7a3][ì„ë¥¼](?:\s|$|[^\uac00-\ud7a3])',  # ~ì„/ë¥¼
            r'[\uac00-\ud7a3][ì€ëŠ”](?:\s|$|[^\uac00-\ud7a3])',  # ~ì€/ëŠ”
            # ë¶€ì‚¬ê²© ì¡°ì‚¬
            r'[\uac00-\ud7a3]ì—ì„œ(?:\s|$)',  # ~ì—ì„œ
            r'[\uac00-\ud7a3]ì—(?:\s|$)',    # ~ì—
            r'[\uac00-\ud7a3]ìœ¼?ë¡œ(?:\s|$)', # ~ìœ¼ë¡œ/ë¡œ
            r'[\uac00-\ud7a3][ì™€ê³¼](?:\s|$)', # ~ì™€/ê³¼
            r'[\uac00-\ud7a3]ì˜(?:\s|$)',    # ~ì˜
            # ì˜ì–´ ë‹¨ì–´ + í•œêµ­ì–´ ì¡°ì‚¬ (ê³ ìœ ëª…ì‚¬ ì²˜ë¦¬)
            r'[A-Za-z]ì—ì„œ(?:\s|$)',         # Recordsì—ì„œ
            r'[A-Za-z]ìœ¼?ë¡œ(?:\s|$)',        # Tabìœ¼ë¡œ
            r'[A-Za-z][ì„ë¥¼](?:\s|$)',       # Tabì„
            r'[A-Za-z][ì´ê°€](?:\s|$)',       # Tabì´
            r'[A-Za-z][ì€ëŠ”](?:\s|$)',       # Tabì€
            r'[A-Za-z]ì™€(?:\s|$)',           # Tabì™€
        ]
        
        korean_particle_count = 0
        for pattern in korean_particle_patterns:
            korean_particle_count += len(re.findall(pattern, original_text))
        
        # 4. í•œêµ­ì–´ ì–´ë¯¸ íŒ¨í„´ (ë¬¸ì¥ ì¢…ê²°)
        korean_ending_patterns = [
            r'ì…ë‹ˆë‹¤[.!?\s]?$', r'ìŠµë‹ˆë‹¤[.!?\s]?$', r'ë©ë‹ˆë‹¤[.!?\s]?$',
            r'ìˆìŠµë‹ˆë‹¤[.!?\s]?$', r'ì—†ìŠµë‹ˆë‹¤[.!?\s]?$', r'í–ˆìŠµë‹ˆë‹¤[.!?\s]?$',
            r'í•©ë‹ˆë‹¤[.!?\s]?$', r'ë©ë‹ˆë‹¤[.!?\s]?$', r'ì§‘ë‹ˆë‹¤[.!?\s]?$',
            r'ì…ë‹ˆê¹Œ[.!?\s]?$', r'ìŠµë‹ˆê¹Œ[.!?\s]?$',
            r'ì„¸ìš”[.!?\s]?$', r'í•´ìš”[.!?\s]?$', r'ë¼ìš”[.!?\s]?$',
            r'[ë‹¤ìŒì„í•¨ë¨ì—†ìŒìˆìŒ][.!?\s]?$',  # ìŒìŠ´ì²´
            r'í˜„ìƒì…ë‹ˆë‹¤', r'í˜„ìƒì„', r'ë°œìƒí•¨', r'í™•ì¸ë¨',
            r'ëŠë¦½ë‹ˆë‹¤', r'ë¹ ë¦…ë‹ˆë‹¤', r'ë§ìŠµë‹ˆë‹¤', r'ì ìŠµë‹ˆë‹¤',
            r'ë©ë‹ˆë‹¤', r'ì•ŠìŠµë‹ˆë‹¤', r'ëª»í•©ë‹ˆë‹¤',
        ]
        
        korean_ending_count = 0
        for pattern in korean_ending_patterns:
            if re.search(pattern, original_text, re.MULTILINE):
                korean_ending_count += 1
        
        # 5. í•œêµ­ì–´ ë¬¸ì¥ êµ¬ì¡° ì ìˆ˜
        korean_structure_score = korean_particle_count + korean_ending_count
        
        # 6. ì˜ì–´ ë¬¸ì¥ íŒ¨í„´ (ê´€ì‚¬, ì „ì¹˜ì‚¬, beë™ì‚¬ ë“±ì´ ë¬¸ì¥ ë‚´ì—ì„œ ì‚¬ìš©ë  ë•Œ)
        english_sentence_patterns = [
            r'\b(the|a|an)\s+\w+',           # ê´€ì‚¬ + ëª…ì‚¬
            r'\b(is|are|was|were|be)\s+',    # beë™ì‚¬
            r'\b(have|has|had)\s+(been|to)', # have + been/to
            r'\b(to|for|from|with|by|at|in|on)\s+\w+',  # ì „ì¹˜ì‚¬ + ëª…ì‚¬
            r'\b(when|where|what|who|why|how)\s+',      # ì˜ë¬¸ì‚¬
            r'\b(if|then|else|because|although)\s+',    # ì ‘ì†ì‚¬
            r'\bshould\s+(be|not|have)',     # should + ë™ì‚¬
            r'\bcan\s+(be|not|have)',        # can + ë™ì‚¬
            r'\bwill\s+(be|not|have)',       # will + ë™ì‚¬
        ]
        
        english_sentence_count = 0
        text_lower = original_text.lower()
        for pattern in english_sentence_patterns:
            english_sentence_count += len(re.findall(pattern, text_lower))
        
        # === íŒë‹¨ ë¡œì§ (ìš°ì„ ìˆœìœ„ ìˆœ) ===
        
        # ìµœìš°ì„ : í•œêµ­ì–´ ì¡°ì‚¬/ì–´ë¯¸ê°€ 1ê°œë¼ë„ ìˆìœ¼ë©´ í•œêµ­ì–´
        if korean_structure_score >= 1:
            return "ko"
        
        # í•œê¸€ì´ ìˆê³ , ì˜ì–´ ë¬¸ì¥ íŒ¨í„´ì´ ì—†ìœ¼ë©´ í•œêµ­ì–´
        if korean_chars >= 1 and english_sentence_count == 0:
            return "ko"
        
        # í•œê¸€ì´ ì˜ì–´ë³´ë‹¤ ë§ìœ¼ë©´ í•œêµ­ì–´
        if korean_chars > latin_chars:
            return "ko"
        
        # ì˜ì–´ ë¬¸ì¥ íŒ¨í„´ì´ ìˆê³  í•œê¸€ì´ ì—†ìœ¼ë©´ ì˜ì–´
        if english_sentence_count >= 1 and korean_chars == 0:
            return "en"
        
        # í•œê¸€ ì—†ê³  ì˜ì–´ê°€ ìˆìœ¼ë©´ ì˜ì–´
        if korean_chars == 0 and latin_chars > 0:
            return "en"
        
        # ê¸°ë³¸ê°’: í•œê¸€ì´ ì¡°ê¸ˆì´ë¼ë„ ìˆìœ¼ë©´ í•œêµ­ì–´ë¡œ (ë³´ìˆ˜ì  ì ‘ê·¼)
        if korean_chars > 0:
            return "ko"
        
        return "unknown"

    def _extract_detectable_text(self, text: str) -> str:
        cleaned = text
        cleaned = re.sub(r"![^!]+!", " ", cleaned)
        cleaned = re.sub(r"\[\^[^\]]+\]", " ", cleaned)
        cleaned = re.sub(r"__.*?__", " ", cleaned)
        cleaned = re.sub(r"\{color:[^}]+\}|\{color\}", " ", cleaned)
        cleaned = re.sub(r"`[^`]+`", " ", cleaned)
        cleaned = re.sub(r"[^A-Za-z\uac00-\ud7a3]", "", cleaned)
        return cleaned

    def _is_bilingual_summary(self, summary: str) -> bool:
        """
        Summaryê°€ ì´ë¯¸ 'í•œê¸€ / ì˜ì–´' ê°™ì´ ì–‘ì–¸ì–´ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ íŒë³„.
        ë¸Œë˜í‚· prefix([Test] [System Menu])ëŠ” ì œì™¸í•˜ê³ , ë‚˜ë¨¸ì§€ core ë¶€ë¶„ë§Œ ê²€ì‚¬í•œë‹¤.
        """
        _, core = self._split_bracket_prefix(summary or "")
        if " / " not in core:
            return False
        left, right = core.split(" / ", 1)
        left_lang = self._detect_text_language(left)
        right_lang = self._detect_text_language(right)
        if left_lang == "unknown" or right_lang == "unknown":
            return False
        return left_lang != right_lang

    def _is_description_already_translated(self, value: str) -> bool:
        """
        Description ë‚´ì— ì´ë¯¸ ë²ˆì—­ ì¤„({color:#4c9aff} ...)ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´
        í•œ ë²ˆ ì´ìƒ ë²ˆì—­ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ë‹¤ì‹œ ë²ˆì—­í•˜ì§€ ì•ŠëŠ”ë‹¤.
        """
        if not value:
            return False
        # ë‹¨ìˆœíˆ íƒœê·¸ë§Œ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼, íƒœê·¸ ì•ˆì— ë‚´ìš©ì´ ìˆê±°ë‚˜ íƒœê·¸ ë’¤ì— ë‚´ìš©ì´ ìˆëŠ” íŒ¨í„´ì„ ì°¾ìŒ
        # ì˜ˆ: {color:#4c9aff}Translation{color}
        # ë‹¨, í…Œì´ë¸” êµ¬ë¶„ì(|)ë§Œ ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸ (ì˜ˆ: {color:#4c9aff}|{color})
        return bool(re.search(r"\{color:#4c9aff\}(?!\s*\|?\s*\{color\}).+", value))

    def _is_steps_bilingual(self, value: str) -> bool:
        """
        customfield_10399(ì¬í˜„ ë‹¨ê³„)ê°€ ì´ë¯¸ 'ì›ë¬¸ ë¸”ë¡ + ë²ˆì—­ ë¸”ë¡' í˜•íƒœì¸ì§€ íŒë³„.
        format_steps_valueì—ì„œ original + '\\n\\n' + translated í˜•íƒœë¡œ ë§Œë“œëŠ” ê²ƒì„ ì´ìš©í•œë‹¤.
        """
        if not value:
            return False
        parts = [p.strip() for p in value.split("\n\n") if p.strip()]
        if len(parts) < 2:
            return False
        first, second = parts[0], parts[1]
        first_lang = self._detect_text_language(first)
        second_lang = self._detect_text_language(second)
        if first_lang == "unknown" or second_lang == "unknown":
            return False
        return first_lang != second_lang

    def _split_bracket_prefix(self, text: str) -> tuple[str, str]:
        """
        Summary ë§¨ ì•ì˜ [System Menu] ê°™ì€ ë¸Œë˜í‚· ë¸”ë¡ì„ ë¶„ë¦¬í•œë‹¤.
        ì˜ˆ) "[Test] [System Menu] ì—ë””í„° ..." -> ("[Test] [System Menu] ", "ì—ë””í„° ...")
        ì—¬ëŸ¬ ê°œì˜ ëŒ€ê´„í˜¸ ë¸”ë¡ì´ ì—°ì†ë˜ëŠ” ê²½ìš°ë„ í—ˆìš©í•œë‹¤.
        """
        if not text:
            return "", ""
        m = re.match(r'^(\s*(?:\[[^\]]*\]\s*)+)(.*)$', text)
        if m:
            return m.group(1), m.group(2)
        return "", text

    def format_summary_value(self, original: str, translated: str) -> str:
        """
        SummaryëŠ” í•œ ì¤„ì´ì–´ì•¼ í•˜ê³  Jira í•„ë“œ ì œí•œì´ 255ìì´ë¯€ë¡œ
        ì›ë¬¸ì€ ê·¸ëŒ€ë¡œ ë‘ê³  ë²ˆì—­ë¬¸ë§Œ ì˜ë¼ì„œ ì œí•œì„ ì§€í‚¨ë‹¤.
        """
        MAX_LEN = 255
        SEPARATOR = " / "

        def _normalize(text: str) -> str:
            return (text or "").replace("\n", " ").strip()

        def _truncate(text: str, limit: int) -> str:
            if limit <= 0:
                return ""
            if len(text) <= limit:
                return text
            if limit == 1:
                return text[:1]
            return text[: limit - 1].rstrip() + "â€¦"

        original = _normalize(original)
        translated = _normalize(translated)

        if not original:
            return _truncate(translated, MAX_LEN)
        if not translated:
            return original

        remaining = MAX_LEN - len(original) - len(SEPARATOR)
        if remaining <= 0:
            return original

        truncated_translated = _truncate(translated, remaining)
        if not truncated_translated:
            return original

        return f"{original}{SEPARATOR}{truncated_translated}"

    def format_steps_value(self, original: str, translated: str) -> str:
        original = (original or "").strip()
        translated = (translated or "").strip()
        if original and translated:
            return f"{original}\n\n{translated}"
        return original or translated

    def build_field_update_payload(self, translation_results: dict[str, dict[str, str]]) -> dict[str, str]:
        payload: dict[str, str] = {}
        for field, content in translation_results.items():
            original = content.get('original', '')
            translated = content.get('translated', '')

            if not translated:
                continue

            if field == "summary":
                formatted = self.format_summary_value(original, translated)
            elif field == "description":
                formatted = translated
            elif field.startswith("customfield_"): # Steps to Reproduce fields
                formatted = self.format_steps_value(original, translated)
            else:
                formatted = translated

            if formatted:
                payload[field] = formatted

        return payload

    def _load_glossary_terms(self, filename: str) -> dict[str, str]:
        """ì§€ì •ëœ ìš©ì–´ì§‘ íŒŒì¼ì—ì„œ terms ë”•ì…”ë„ˆë¦¬ë¥¼ ë¡œë“œ."""
        try:
            base_dir = Path(__file__).resolve().parent
            glossary_path = base_dir / filename
            if not glossary_path.exists():
                return {}

            with glossary_path.open("r", encoding="utf-8") as f:
                data = json.load(f)

            terms = data.get("terms") or {}
            if not isinstance(terms, dict):
                return {}
            return terms
        except Exception:
            # ìš©ì–´ì§‘ì´ ì—†ì–´ë„ ë²ˆì—­ì€ ë™ì‘í•´ì•¼ í•˜ë¯€ë¡œ ì¡°ìš©íˆ ë¬´ì‹œ
            return {}

    def _call_openai_batch(
        self,
        chunks: Sequence[TranslationChunk],
        retries: int = 2,
    ) -> dict[str, str]:
        if not chunks:
            return {}

        attempt = 0
        last_error: Optional[Exception] = None
        batch_result: dict[str, str] = {}

        while attempt <= retries:
            attempt += 1
            try:
                batch_result = self._call_openai_batch_once(chunks)
                break
            except Exception as exc:
                last_error = exc
                if attempt > retries:
                    break
                print(f"âš ï¸ Batch translation failed (attempt {attempt}/{retries + 1}): {exc}")

        if not batch_result and last_error:
            raise last_error

        missing_ids = [chunk.id for chunk in chunks if chunk.id not in batch_result]
        if missing_ids:
            print(f"âš ï¸ Batch translation missing {len(missing_ids)} chunk(s); retrying individually.")
            missing_chunks = [chunk for chunk in chunks if chunk.id in missing_ids]
            fallback = self._translate_chunk_list(missing_chunks)
            batch_result.update(fallback)

        return batch_result

    def _call_openai_batch_once(
        self,
        chunks: Sequence[TranslationChunk],
    ) -> dict[str, str]:
        """
        OpenAI Structured Outputs(beta.parse)ë¥¼ ì‚¬ìš©í•˜ì—¬
        JSON íŒŒì‹± ì—ëŸ¬ë¥¼ ì›ì²œ ì°¨ë‹¨í•˜ê³  ì•ˆì •ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.
        í•œê¸€â†’ì˜ì–´, ì˜ì–´â†’í•œê¸€ ìë™ ë²ˆì—­.
        """
        if not chunks:
            return {}

        glossary_instruction = self._build_glossary_instruction(
            [chunk.clean_text for chunk in chunks],
        )
        
        # ì²­í¬ë“¤ì˜ ì£¼ìš” ì–¸ì–´ ê°ì§€ (ì²« ë²ˆì§¸ ì²­í¬ ê¸°ì¤€)
        combined_text = "\n".join(chunk.clean_text for chunk in chunks)
        detected_lang = self._detect_text_language(combined_text)
        
        if detected_lang == "ko":
            # í•œêµ­ì–´ â†’ ì˜ì–´: ì™„ì „í•œ ì˜ì–´ë¡œ ë²ˆì—­
            system_msg = (
                "You are a professional Korean to English translator. "
                "Translate each provided Korean text to English. "
                "The output MUST be 100% in English - do NOT leave any Korean words. "
                "Preserve Jira markup (*bold*, _italic_, {{code}}, etc.), bullet indentation, "
                "and placeholder tokens like __IMAGE_PLACEHOLDER__. "
                "IMPORTANT: Keep the exact same number of lines as the source text. "
                "Do not add commentary."
            )
        else:
            # ì˜ì–´ â†’ í•œêµ­ì–´: ê³ ìœ ëª…ì‚¬ëŠ” ì˜ì–´ ìœ ì§€
            system_msg = (
                "You are a professional English to Korean translator. "
                "Translate each provided English text to Korean. "
                "Keep proper nouns and game-specific terms in English. "
                "Use terse memo-style (ìŒìŠ´ì²´) and concise noun phrases for titles/summaries. "
                "Preserve Jira markup (*bold*, _italic_, {{code}}, etc.), bullet indentation, "
                "and placeholder tokens like __IMAGE_PLACEHOLDER__. "
                "IMPORTANT: Keep the exact same number of lines as the source text. "
                "Do not add commentary."
            )
        
        if glossary_instruction:
            system_msg = f"{system_msg} {glossary_instruction}"

        # Structured Outputsìš© í˜ì´ë¡œë“œ êµ¬ì„±
        payload = {
            "items": [
                {"id": chunk.id, "text": chunk.clean_text}
                for chunk in chunks
            ]
        }
        user_msg = (
            f"Translate the text fields in the following JSON data. Keep 'id' unchanged.\n"
            f"{json.dumps(payload, ensure_ascii=False)}"
        )

        # client.beta.chat.completions.parse ì‚¬ìš©
        # response_formatì— Pydantic í´ë˜ìŠ¤ë¥¼ ë„˜ê²¨ì¤ë‹ˆë‹¤.
        completion = self.openai.beta.chat.completions.parse(
            model=self.openai_model,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            response_format=TranslationResponse,
        )

        # íŒŒì‹±ëœ ê²°ê³¼(parsed)ë¥¼ ë°”ë¡œ ì‚¬ìš©
        parsed_response = completion.choices[0].message.parsed
        
        if not parsed_response or not parsed_response.translations:
            # ë§¤ìš° ë“œë¬¸ ê²½ìš°ì§€ë§Œ ê²°ê³¼ê°€ ì—†ì„ ë•Œë¥¼ ëŒ€ë¹„
            raise ValueError("Translation returned no structured data.")

        result: dict[str, str] = {}
        for item in parsed_response.translations:
            if item.id and item.translated:
                result[item.id] = item.translated.strip()

        return result

    def _create_translation_chunk(
        self,
        *,
        chunk_id: str,
        field: str,
        original_text: str,
        header: Optional[str] = None,
    ) -> Optional[TranslationChunk]:
        if original_text is None:
            return None

        attachments, clean_text = self.extract_attachments_markup(original_text)
        return TranslationChunk(
            id=chunk_id,
            field=field,
            original_text=original_text,
            clean_text=clean_text,
            attachments=attachments,
            header=header,
        )

    def _plan_field_translation_job(
        self,
        field: str,
        value: str,
    ) -> Optional[FieldTranslationJob]:
        if not value:
            return None

        if field == "summary":
            _, core = self._split_bracket_prefix(value)
            if not core.strip():
                return None
            chunk = self._create_translation_chunk(
                chunk_id="summary",
                field=field,
                original_text=core,
            )
            if not chunk:
                return None
            return FieldTranslationJob(
                field=field,
                original_value=value,
                chunks=[chunk],
            )

        if field == "description":
            sections = self._extract_description_sections(value)
            chunks: list[TranslationChunk] = []
            if sections:
                for idx, (header, content) in enumerate(sections):
                    if not content.strip():
                        continue
                    chunk = self._create_translation_chunk(
                        chunk_id=f"{field}__section_{idx}",
                        field=field,
                        original_text=content,
                        header=header,
                    )
                    if chunk:
                        chunks.append(chunk)
            else:
                chunk = self._create_translation_chunk(
                    chunk_id=f"{field}__full",
                    field=field,
                    original_text=value,
                )
                if chunk:
                    chunks.append(chunk)
            if not chunks:
                return None
            return FieldTranslationJob(
                field=field,
                original_value=value,
                chunks=chunks,
                mode="description",
            )

        chunk = self._create_translation_chunk(
            chunk_id=field,
            field=field,
            original_text=value,
        )
        if not chunk:
            return None
        return FieldTranslationJob(
            field=field,
            original_value=value,
            chunks=[chunk],
        )
            
    def _format_bilingual_block(self, original: str, translated: str, header: Optional[str] = None) -> str:
        original = (original or "").strip("\n")
        translated = (translated or "").strip()
        
        lines: list[str] = []
        if header:
            lines.append(header)
            
        if not original:
            if translated:
                lines.append(f"{{color:#4c9aff}}{translated}{{color}}")
            return "\n".join(lines).strip()

        # ë²ˆì—­ë¬¸ ë¼ì¸ ì¤€ë¹„
        # ì›ë¬¸ì˜ ì½”ë“œë¸”ëŸ­ ìƒíƒœë¥¼ ì¶”ì í•˜ì—¬ ë²ˆì—­ë¬¸ì—ì„œë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
        translation_source_lines = []
        in_code_block_trans = False
        
        # ì›ë¬¸ì„ ë¨¼ì € ìŠ¤ìº”í•˜ì—¬ ë²ˆì—­ ê°€ëŠ¥í•œ ë¼ì¸ë§Œ ì¶”ì¶œ
        original_lines_for_scan = original.splitlines()
        translatable_original_lines = []
        in_code_block_orig = False
        
        for orig_line in original_lines_for_scan:
            is_code_line, in_code_block_orig = self._is_inside_code_block(orig_line, in_code_block_orig)
            if is_code_line or in_code_block_orig:
                continue  # ì½”ë“œë¸”ëŸ­ ë¼ì¸ì€ ì œì™¸
            
            stripped_orig = orig_line.strip()
            if not stripped_orig:
                continue
            
            # ë¯¸ë””ì–´, í—¤ë” ë¼ì¸ì€ ë²ˆì—­ ë§¤ì¹­ì—ì„œ ì œì™¸ (í‘œëŠ” í¬í•¨)
            if self._is_media_line(stripped_orig) or self._is_header_line(stripped_orig):
                continue
            
            translatable_original_lines.append(orig_line)
        
        # ë²ˆì—­ë¬¸ì—ì„œë„ ì½”ë“œë¸”ëŸ­ ë¼ì¸ ì œì™¸í•˜ê³  ë²ˆì—­ ê°€ëŠ¥í•œ ë¼ì¸ë§Œ ì¶”ì¶œ
        for trans_line in translated.splitlines():
            is_code_line, in_code_block_trans = self._is_inside_code_block(trans_line, in_code_block_trans)
            if is_code_line or in_code_block_trans:
                continue  # ì½”ë“œë¸”ëŸ­ ë¼ì¸ì€ ì œì™¸
            
            trans_stripped = trans_line.strip()
            if not trans_stripped:
                continue
            
            # ë¯¸ë””ì–´, í—¤ë” ë¼ì¸ì€ ë²ˆì—­ ë§¤ì¹­ì—ì„œ ì œì™¸
            if self._is_media_line(trans_stripped) or self._is_header_line(trans_stripped):
                continue
            
            translation_source_lines.append(trans_line)
            
        translation_index = 0

        def next_translation_line() -> str:
            nonlocal translation_index
            if translation_index < len(translation_source_lines):
                line = translation_source_lines[translation_index]
                translation_index += 1
                return line
            return ""

        # í…ìŠ¤íŠ¸ ë²„í¼ (ë¯¸ë””ì–´ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ëª¨ì•„ë‘ )
        text_buffer: list[str] = []
        
        def flush_text_buffer():
            nonlocal text_buffer
            if not text_buffer:
                return
            
            # 1. ì›ë¬¸ í…ìŠ¤íŠ¸ ì¶œë ¥
            lines.extend(text_buffer)
            
            # 2. ë²ˆì—­ë¬¸ í…ìŠ¤íŠ¸ ì¶œë ¥ (ì›ë¬¸ ë°”ë¡œ ì•„ë˜)
            # ì›ë¬¸ ë¼ì¸ ìˆ˜ë§Œí¼ ë²ˆì—­ë¬¸ì„ ê°€ì ¸ì™€ì„œ í¬ë§·íŒ…
            translated_block = []
            for org_line in text_buffer:
                stripped = org_line.strip()
                if not stripped:
                    continue
                
                translated_line = next_translation_line().strip()
                if translated_line:
                    formatted = self._match_translated_line_format(org_line, translated_line)
                    if formatted:
                        translated_block.append(formatted)
            
            if translated_block:
                # ì›ë¬¸ê³¼ ë²ˆì—­ ë¸”ë¡ ì‚¬ì´ì— ë¹ˆ ì¤„ ì¶”ê°€
                lines.append("")
                lines.extend(translated_block)
            
            text_buffer = []

        original_lines = original.splitlines()
        in_code_block = False
        
        for line in original_lines:
            stripped = line.strip()
            
            # ì½”ë“œë¸”ëŸ­ ìƒíƒœ ì—…ë°ì´íŠ¸
            is_code_line, in_code_block = self._is_inside_code_block(line, in_code_block)
            
            # ì½”ë“œë¸”ëŸ­ ë‚´ë¶€ ë˜ëŠ” ì½”ë“œë¸”ëŸ­ íƒœê·¸ ë¼ì¸ì€ ìŠ¤í‚µ
            if is_code_line or in_code_block:
                flush_text_buffer()  # ì½”ë“œë¸”ëŸ­ ë‚˜ì˜¤ê¸° ì „ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                lines.append(line)  # ì½”ë“œë¸”ëŸ­ ë¼ì¸ ì¶œë ¥
                continue
            
            # í…Œì´ë¸” ë¼ì¸ ì²˜ë¦¬ (|ë¡œ ì‹œì‘í•˜ê³  |ë¡œ ëë‚˜ëŠ” ê²½ìš°)
            if stripped.startswith("|") and stripped.endswith("|"):
                flush_text_buffer() # í…Œì´ë¸” ë‚˜ì˜¤ê¸° ì „ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                # Jiraê°€ í‘œë¥¼ ì œëŒ€ë¡œ ë Œë”ë§í•˜ë ¤ë©´ ì•ì— ë¹ˆ ì¤„ì´ í•„ìš”
                lines.append("")
                
                # ë²ˆì—­ëœ í‘œ ë¼ì¸ ê°€ì ¸ì˜¤ê¸° (LLMì´ í‘œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë¼ì¸ìœ¼ë¡œ ë²ˆì—­)
                translated_table_line = next_translation_line()
                
                # í—¤ë” ì…€ (||)ê³¼ ë°ì´í„° ì…€ (|) êµ¬ë¶„
                is_header_row = line.strip().startswith("||")
                
                if is_header_row:
                    # í—¤ë” í–‰ ì²˜ë¦¬
                    orig_cells = line.split("||")
                    trans_cells = translated_table_line.split("||") if translated_table_line else []
                    
                    new_cells = []
                    for i, orig_cell in enumerate(orig_cells):
                        # split ê²°ê³¼ì˜ ì²«ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ì€ ë¹ˆ ë¬¸ìì—´
                        if i == 0 or i == len(orig_cells) - 1:
                            new_cells.append(orig_cell)
                            continue
                        
                        # ì›ë¬¸ ì…€ì—ì„œ ë³„í‘œ ì œê±°í•˜ì—¬ ì‹¤ì œ ë‚´ìš© ì¶”ì¶œ
                        orig_content = orig_cell.strip().strip("*").strip()
                        if not orig_content:
                            new_cells.append(orig_cell)
                            continue
                        
                        # ëŒ€ì‘í•˜ëŠ” ë²ˆì—­ ì…€ ê°€ì ¸ì˜¤ê¸°
                        if trans_cells and i < len(trans_cells):
                            trans_content = trans_cells[i].strip().strip("*").strip()
                            if trans_content:
                                # í¬ë§·: "*ì›ë¬¸/ë²ˆì—­*"
                                new_cells.append(f"*{orig_content}/{trans_content}*")
                            else:
                                new_cells.append(orig_cell)
                        else:
                            new_cells.append(orig_cell)
                    
                    lines.append("||".join(new_cells))
                else:
                    # ë°ì´í„° í–‰ ì²˜ë¦¬
                    orig_cells = line.split("|")
                    trans_cells = translated_table_line.split("|") if translated_table_line else []
                    
                    new_cells = []
                    for i, orig_cell in enumerate(orig_cells):
                        # split ê²°ê³¼ì˜ ì²«ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ì€ ë¹ˆ ë¬¸ìì—´
                        if i == 0 or i == len(orig_cells) - 1:
                            new_cells.append(orig_cell)
                            continue
                        
                        orig_content = orig_cell.strip()
                        if not orig_content:
                            new_cells.append(orig_cell)
                            continue
                        
                        # ì…€ ë‚´ìš©ì´ ë¯¸ë””ì–´ì¸ ê²½ìš° ë²ˆì—­ ìŠ¤í‚µ
                        if self._is_media_line(orig_content):
                            new_cells.append(orig_cell)
                            continue
                        
                        # ëŒ€ì‘í•˜ëŠ” ë²ˆì—­ ì…€ ê°€ì ¸ì˜¤ê¸°
                        if trans_cells and i < len(trans_cells):
                            trans_content = trans_cells[i].strip()
                            if trans_content and not self._is_media_line(trans_content):
                                # í¬ë§·: "ì›ë¬¸/ë²ˆì—­"
                                new_cells.append(f"{orig_content}/{trans_content}")
                            else:
                                new_cells.append(orig_cell)
                        else:
                            new_cells.append(orig_cell)
                    
                    lines.append("|".join(new_cells))
                continue

            # ë¯¸ë””ì–´ ë¼ì¸ ì²˜ë¦¬
            if self._is_media_line(stripped):
                flush_text_buffer() # ë¯¸ë””ì–´ ë‚˜ì˜¤ê¸° ì „ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                lines.append(line) # ë¯¸ë””ì–´ ë¼ì¸ ì¶œë ¥
                continue
            
            # í—¤ë” ë¼ì¸ ì²˜ë¦¬
            if self._is_header_line(stripped):
                flush_text_buffer()
                lines.append(line)
                continue

            # ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” ë²„í¼ì— ì¶”ê°€
            text_buffer.append(line)

        flush_text_buffer() # ë‚¨ì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            
        return "\n".join(lines).strip()

    def _match_translated_line_format(self, original_line: str, translated_line: str) -> str:
        translation = translated_line.strip()
        if not translation:
            return ""

        # ì›ë¬¸ì— ìƒ‰ìƒ íƒœê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸
        has_color_tag = "{color:" in original_line or "{color}" in original_line
        
        # ì›ë¬¸ì˜ ë“¤ì—¬ì“°ê¸° ë° ë¶ˆë¦¿/ë²ˆí˜¸ íŒ¨í„´ ê°ì§€
        # ì˜ˆ: "  - Item" -> prefix="  - "
        # ì˜ˆ: "    1. Item" -> prefix="    1. "
        match = re.match(r"^(\s*(?:[-*#]+|\d+\.)\s+)(.*)", original_line)
        if match:
            prefix = match.group(1)
            cleaned_translation = self._strip_bullet_prefix(translation)
            # ì›ë¬¸ì˜ prefix êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê³ , ë‚´ìš©ë§Œ ìƒ‰ìƒ ì²˜ë¦¬ (ì›ë¬¸ì— ìƒ‰ìƒ íƒœê·¸ê°€ ì—†ì„ ë•Œë§Œ)
            if not has_color_tag:
                return f"{prefix}{{color:#4c9aff}}{cleaned_translation}{{color}}"
            else:
                return f"{prefix}{cleaned_translation}"
        
        # ë¶ˆë¦¿ì´ ì—†ëŠ” ê²½ìš° (ì¼ë°˜ í…ìŠ¤íŠ¸)
        # ì›ë¬¸ì˜ leading whitespaceë¥¼ ê°ì§€í•˜ì—¬ ë²ˆì—­ë¬¸ì—ë„ ì ìš©
        indent_match = re.match(r"^(\s*)", original_line)
        if indent_match:
            indent = indent_match.group(1)
            if not has_color_tag:
                return f"{indent}{{color:#4c9aff}}{translation}{{color}}"
            else:
                return f"{indent}{translation}"
        
        # ê¸°ë³¸ ì¼€ì´ìŠ¤
        if not has_color_tag:
            return f"{{color:#4c9aff}}{translation}{{color}}"
        else:
            return translation

    def _strip_bullet_prefix(self, text: str) -> str:
        return re.sub(r"^\s*(?:[-*#]+|\d+[\.\)])\s*", "", text).strip()

    def _is_media_line(self, stripped_line: str) -> bool:
        if not stripped_line:
            return False

        def _strip_bullet_prefix(text: str) -> str:
            return re.sub(r"^\s*(?:[-*#]+|\d+[\.\)])\s*", "", text or "").strip()

        candidates = [stripped_line, _strip_bullet_prefix(stripped_line)]

        for candidate in candidates:
            if not candidate:
                continue
            if candidate.startswith("!"):
                return True
            if candidate.startswith("[^"):
                return True
            if candidate.startswith("["):
                return True
            # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° íŒ¨í„´ ê°ì§€ (ì˜ˆ: width=...,height=...,alt="..."!)
            if re.search(r'(width|height|alt)=.*!$', candidate):
                return True
        if "__IMAGE_PLACEHOLDER" in stripped_line or "__ATTACHMENT_PLACEHOLDER" in stripped_line:
            return True
        return False
    
    def _is_code_block_line(self, line: str) -> bool:
        """
        ì½”ë“œë¸”ëŸ­ íƒœê·¸ ë¼ì¸ì¸ì§€ íŒë‹¨.
        - {code} ë˜ëŠ” {code:language}ê°€ í¬í•¨ëœ ë¼ì¸
        - {noformat}ì´ í¬í•¨ëœ ë¼ì¸
        """
        if not line:
            return False
        stripped = line.strip()
        
        # noformat íƒœê·¸ ê°ì§€
        if "{noformat}" in stripped:
            return True
            
        # code íƒœê·¸ ê°ì§€
        if re.search(r'\{code(?::[^}]*)?\}', stripped):
            return True
            
        return False

    def _is_inside_code_block(self, line: str, in_code_block: bool) -> tuple[bool, bool]:
        """
        ì½”ë“œë¸”ëŸ­ ë‚´ë¶€ì¸ì§€ íŒë‹¨í•˜ê³  ìƒíƒœ ì—…ë°ì´íŠ¸.
        {code}, {code:language}, {noformat} ë¸”ë¡ì„ ëª¨ë‘ ì²˜ë¦¬.
        íƒœê·¸ê°€ í•œ ì¤„ì— í™€ìˆ˜ ê°œ ìˆìœ¼ë©´ ìƒíƒœë¥¼ í† ê¸€í•©ë‹ˆë‹¤.
        
        Args:
            line: í˜„ì¬ ë¼ì¸
            in_code_block: í˜„ì¬ ì½”ë“œë¸”ëŸ­ ë‚´ë¶€ ìƒíƒœ
            
        Returns:
            (is_code_line, new_in_code_block_state)
            - is_code_line: ì´ ë¼ì¸ì´ ì½”ë“œë¸”ëŸ­ íƒœê·¸ ë¼ì¸ì¸ì§€ ì—¬ë¶€
            - new_in_code_block_state: ì—…ë°ì´íŠ¸ëœ ì½”ë“œë¸”ëŸ­ ë‚´ë¶€ ìƒíƒœ
        """
        if not line:
            return False, in_code_block
        
        stripped = line.strip()
        
        # 1. {noformat} ì²˜ë¦¬
        if "{noformat}" in stripped:
            # íƒœê·¸ ê°œìˆ˜ ì„¸ê¸°
            count = stripped.count("{noformat}")
            # í™€ìˆ˜ ê°œë©´ ìƒíƒœ í† ê¸€ (ì—´ê±°ë‚˜ ë‹«ìŒ)
            if count % 2 == 1:
                return True, not in_code_block
            # ì§ìˆ˜ ê°œë©´ (ì—´ê³  ë‹«í˜) í•´ë‹¹ ë¼ì¸ì€ ì½”ë“œ ë¼ì¸ì´ì§€ë§Œ ë¸”ë¡ ìƒíƒœëŠ” ìœ ì§€
            return True, in_code_block
            
        # 2. {code} ì²˜ë¦¬
        # {code} ë˜ëŠ” {code:xxx} íƒœê·¸ ì°¾ê¸°
        code_tags = re.findall(r'\{code(?::[^}]*)?\}', stripped)
        if code_tags:
            # íƒœê·¸ ê°œìˆ˜ê°€ í™€ìˆ˜ë©´ ìƒíƒœ í† ê¸€
            if len(code_tags) % 2 == 1:
                return True, not in_code_block
            return True, in_code_block
        
        # ì½”ë“œë¸”ëŸ­ ë‚´ë¶€ ìƒíƒœ ìœ ì§€
        return in_code_block, in_code_block
    
    def _is_header_line(self, line: str) -> bool:
        """
        ì´ ì¤„ì´ ì„¹ì…˜ í—¤ë”(Observed / Expected / Note / Video ë“±)ì¸ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨.
        ì˜ì–´-only ë¼ë²¨ê³¼ ì˜ì–´/êµ­ë¬¸ í˜¼í•© ë¼ë²¨(ì˜ˆ: 'Expected/ê¸°ëŒ€ ê²°ê³¼:')ì„ ëª¨ë‘ í—¤ë”ë¡œ ì·¨ê¸‰í•œë‹¤.
        """
        cleaned = re.sub(r"\{color:[^}]+\}|\{color\}", "", line or "").strip()
        return self._match_section_header(cleaned) is not None

    def _extract_description_sections(self, text: str) -> list[tuple[Optional[str], str]]:
        if not text:
            return []

        sections: list[tuple[Optional[str], str]] = []
        current_header: Optional[str] = None
        buffer: list[str] = []

        def flush():
            nonlocal buffer
            if not buffer:
                return
            content = "\n".join(buffer).strip("\n")
            buffer = []
            if content:
                sections.append((current_header, content))

        for line in text.splitlines():
            header = self._match_section_header(line)
            if header:
                flush()
                current_header = header
                continue
            buffer.append(line)
        flush()

        return sections

    def _match_section_header(self, line: str) -> Optional[str]:
        """
        Description ë‚´ì—ì„œ ì„¹ì…˜ í—¤ë”(Observed, Expected, Note, Video ë“±)ë¥¼ ì°¾ì•„ì„œ
        ë§¤ì¹­ë˜ëŠ” ê²½ìš° ì›ë˜ ë¼ë²¨(ì˜ì–´/êµ­ë¬¸ í˜¼í•© í¬í•¨)ì„ ë°˜í™˜í•œë‹¤.

        ì˜ˆ:
            "Expected Result:"           -> "Expected Result:"
            "Expected/ê¸°ëŒ€ ê²°ê³¼:"        -> "Expected/ê¸°ëŒ€ ê²°ê³¼:"
            "Video/ì˜ìƒ:"                -> "Video/ì˜ìƒ:"
        """
        # ìƒ‰ìƒ/ìŠ¤íƒ€ì¼ ë§ˆí¬ì—… ì œê±°
        stripped = re.sub(r"\{color:[^}]+\}|\{color\}", "", line or "").strip()
        # ë§ˆì§€ë§‰ ì½œë¡  ì œê±° ë° ì–‘ë * / _ ì œê±° (ë§¤ì¹­ ìš©ë„ë¡œë§Œ ì‚¬ìš©)
        stripped_no_colon = stripped.rstrip(":").strip("*_ ")
        lowered = stripped_no_colon.lower()

        # í˜¼í•© ë¼ë²¨ì—ì„œ ì•ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì˜ˆ: "expected/ê¸°ëŒ€ ê²°ê³¼", "observed(ê´€ì°° ê²°ê³¼)" ë“±)
        if "/" in lowered:
            left = lowered.split("/", 1)[0].strip()
        else:
            left = lowered
        # ê´„í˜¸ë‚˜ ì¶”ê°€ ì„¤ëª…ì´ ë¶™ì–´ë„ ì•ë¶€ë¶„ë§Œ ë¹„êµí•˜ë„ë¡ ì¡°ì •
        left = re.split(r"[\(\[]", left, 1)[0].strip()

        for header in self.DESCRIPTION_SECTIONS:
            normalized = header.lower()
            # "expected" ë˜ëŠ” "expected result" í˜•íƒœ ëª¨ë‘ í—ˆìš©
            if left == normalized or left.startswith(f"{normalized} "):
                # ì›ë³¸ í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ì½œë¡  í¬í•¨)
                return stripped

        return None

    def normalize_field_value(self, value) -> str:
        if value is None:
            return ""
        if isinstance(value, str):
            return value.strip()
        if isinstance(value, dict):
            return self._flatten_adf_node(value).strip()
        if isinstance(value, Sequence):
            flattened = "\n".join(
                filter(None, (self.normalize_field_value(item) for item in value))
            )
            return flattened.strip()
        return str(value).strip()

    def _flatten_adf_node(self, node) -> str:
        if isinstance(node, dict):
            node_type = node.get("type")
            if node_type == "text":
                return node.get("text", "")
            if node_type == "hardBreak":
                return "\n"
            content = node.get("content", [])
            text = "".join(self._flatten_adf_node(child) for child in content)
            if node_type in {"paragraph", "heading"} and text:
                return f"{text}\n"
            return text
        if isinstance(node, list):
            return "".join(self._flatten_adf_node(child) for child in node)
        return ""

    def fetch_issue_fields(
        self,
        issue_key: str,
        fields_to_fetch: Optional[Sequence[str]] = None
    ) -> dict[str, str]:
        if not fields_to_fetch:
            # ê¸°ë³¸ê°’ì€ í˜¸ì¶œí•˜ëŠ” ìª½ì—ì„œ ê²°ì •í•´ì„œ ë„˜ê²¨ì£¼ë„ë¡ ë³€ê²½ë¨
            # í•˜ì§€ë§Œ ì•ˆì „ì¥ì¹˜ë¡œ ë‚¨ê²¨ë‘ 
            fields_to_fetch = ["summary", "description"]

        endpoint = f"{self.jira_url}/rest/api/2/issue/{issue_key}"
        params = {
            "fields": ",".join(fields_to_fetch),
            "expand": "renderedFields"
        }

        response = self.session.get(endpoint, params=params, timeout=15)
        response.raise_for_status()
        data = response.json()

        fetched_fields: dict[str, str] = {}
        raw_fields = data.get("fields", {}) or {}
        rendered_fields = data.get("renderedFields", {}) or {}

        for field in fields_to_fetch:
            raw_value = raw_fields.get(field)
            normalized = self.normalize_field_value(raw_value)

            if not normalized:
                rendered_value = rendered_fields.get(field)
                normalized = self.normalize_field_value(rendered_value)

            if normalized:
                fetched_fields[field] = normalized

        return fetched_fields

    def update_issue_fields(self, issue_key: str, field_payload: dict[str, str]) -> None:
        if not field_payload:
            print("â„¹ï¸ ì—…ë°ì´íŠ¸í•  í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        endpoint = f"{self.jira_url}/rest/api/2/issue/{issue_key}"
        response = self.session.put(endpoint, json={"fields": field_payload}, timeout=15)
        
        # ğŸ‘‡ [ì¶”ê°€] ì—ëŸ¬ ë°œìƒ ì‹œ ìƒì„¸ ì‘ë‹µ ë‚´ìš© ì¶œë ¥
        if not response.ok:
            print(f"âŒ Jira API Error ({response.status_code})")
            print(f"Response: {response.text}")
            
        response.raise_for_status()
        print("âœ… Jira ì´ìŠˆê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def translate_issue(
        self,
        issue_key: str,
        fields_to_translate: Optional[list[str]] = None,
        perform_update: bool = False
    ) -> dict:
        """
        Jira ì´ìŠˆë¥¼ ë²ˆì—­ (í•œê¸€â†’ì˜ì–´, ì˜ì–´â†’í•œê¸€ ìë™ ë²ˆì—­)

        Args:
            issue_key: Jira ì´ìŠˆ í‚¤ (ì˜ˆ: 'BUG-123')
            fields_to_translate: ë²ˆì—­í•  í•„ë“œ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: None -> ìë™ ê²°ì •)

        Returns:
            ë²ˆì—­ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # 1. í‹°ì¼“ íƒ€ì… íŒë³„ ë° ì„¤ì •
        if issue_key.upper().startswith("PUBG-"):
            steps_field = "customfield_10237"
            glossary_file = "pubg_glossary.json"
            self.glossary_name = "PUBG"
        elif issue_key.upper().startswith("PAYDAY-"):
            steps_field = "customfield_10237"
            glossary_file = "heist_glossary.json"
            self.glossary_name = "HeistRoyale"
        else:
            # ê¸°ë³¸ê°’ì€ PBB (P2-*)
            steps_field = "customfield_10399"
            glossary_file = "pbb_glossary.json"
            self.glossary_name = "PBB(Project Black Budget)"

        # ìš©ì–´ì§‘ ë¡œë“œ
        self.glossary_terms = self._load_glossary_terms(glossary_file)

        if fields_to_translate is None:
            fields_to_translate = ['summary', 'description', steps_field]

        # 2. ì´ìŠˆ ì¡°íšŒ
        print(f"ğŸ“¥ Fetching issue {issue_key}...")
        issue_fields = self.fetch_issue_fields(issue_key, fields_to_translate)

        if not issue_fields:
            print(f"âš ï¸ No fields found for {issue_key}")
            return {"results": {}, "update_payload": {}, "updated": False, "error": "no_fields"}

        # 3. ê° í•„ë“œë¥¼ ë‹¨ì¼ ë°°ì¹˜ë¡œ ë²ˆì—­ ì¤€ë¹„
        translation_results: dict[str, dict[str, str]] = {}
        jobs: dict[str, FieldTranslationJob] = {}
        all_chunks: list[TranslationChunk] = []

        for field in fields_to_translate:
            field_value = issue_fields.get(field)
            if not field_value:
                continue

            translation_results[field] = {
                "original": field_value,
                "translated": "",
            }

            print(f"ğŸ”„ Translating {field}...")
            skip_reason = None
            if field == "description" and self._is_description_already_translated(field_value):
                skip_reason = "already translated"
            elif field == "summary" and self._is_bilingual_summary(field_value):
                skip_reason = "already bilingual"
            elif field == steps_field and self._is_steps_bilingual(field_value):
                skip_reason = "already bilingual steps"

            if skip_reason:
                print(f"â­ï¸ Skipping {field} ({skip_reason})")
                continue

            job = self._plan_field_translation_job(field, field_value)
            if not job:
                continue

            jobs[field] = job
            all_chunks.extend(job.chunks)

        chunk_translations: dict[str, str] = {}
        if all_chunks:
            try:
                chunk_translations = self._call_openai_batch(all_chunks)
            except Exception as exc:
                print(f"âš ï¸ Batch translation failed, falling back to per-field mode: {exc}")
                chunk_translations = self._translate_chunks_individually(jobs)

        for field, job in jobs.items():
            assembled: list[str] = []
            for chunk in job.chunks:
                translated_raw = chunk_translations.get(chunk.id, "")
                restored = self.restore_attachments_markup(translated_raw, chunk.attachments)
                if job.mode == "description":
                    block = self._format_bilingual_block(
                        chunk.original_text,
                        restored,
                        header=chunk.header,
                    )
                    if block:
                        assembled.append(block)
                else:
                    if restored:
                        assembled.append(restored)
            if job.mode == "description":
                translated_value = "\n\n".join(filter(None, assembled)).strip()
            else:
                translated_value = "\n\n".join(filter(None, assembled))
            translation_results[field]["translated"] = translated_value

        payload = self.build_field_update_payload(translation_results)
        updated = False
        error = None
        if perform_update and payload:
            try:
                self.update_issue_fields(issue_key, payload)
                updated = True
            except Exception as exc:
                error = str(exc)

        return {
            "results": translation_results,
            "update_payload": payload,
            "updated": updated,
            "error": error,
        }


def parse_issue_url(issue_url: str) -> tuple[str, str]:
    parsed = urlparse(issue_url.strip())

    if not parsed.scheme or not parsed.netloc:
        raise ValueError("ìœ íš¨í•œ Jira ì´ìŠˆ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    base_url = f"{parsed.scheme}://{parsed.netloc}"
    path_segments = [segment for segment in parsed.path.split("/") if segment]

    issue_key = None
    if "browse" in path_segments:
        browse_index = path_segments.index("browse")
        if browse_index + 1 < len(path_segments):
            issue_key = path_segments[browse_index + 1]
    if not issue_key:
        match = re.search(r"[A-Z][A-Z0-9]+-\d+", parsed.path, re.IGNORECASE)
        if match:
            issue_key = match.group(0).upper()

    if not issue_key:
        raise ValueError("URLì—ì„œ Jira ì´ìŠˆ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return base_url, issue_key


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":

    load_dotenv()
    # ì„¤ì •
    JIRA_URL = os.getenv("JIRA_URL", "https://cloud.jira.krafton.com").rstrip("/")
    JIRA_EMAIL = os.getenv("JIRA_EMAIL")
    JIRA_API_TOKEN = os.getenv("JIRA_API_TOKEN")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    if not all([JIRA_EMAIL, JIRA_API_TOKEN, OPENAI_API_KEY]):
        raise EnvironmentError("JIRA_EMAIL, JIRA_API_TOKEN, OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ëª¨ë‘ ì„¤ì •í•´ì£¼ì„¸ìš”.")

    default_url = "https://cloud.jira.krafton.com/browse/P2-70735"
    issue_url_input = input(f"ë²ˆì—­í•  Jira í‹°ì¼“ URLì„ ì…ë ¥í•˜ì„¸ìš” (Enter for default: {default_url}): ").strip()
    if not issue_url_input:
        issue_url_input = default_url
        print(f"â„¹ï¸ ê¸°ë³¸ URLì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {issue_url_input}")

    input_base_url, issue_key = parse_issue_url(issue_url_input)
    if JIRA_URL and JIRA_URL.lower() != input_base_url.lower():
        print(f"â„¹ï¸ ì…ë ¥ëœ URLì˜ Jira ì„œë²„({input_base_url})ê°€ ì„¤ì •ëœ ê¸°ë³¸ URL({JIRA_URL})ê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ê¸°ë³¸ URLì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # ë²ˆì—­ê¸° ì´ˆê¸°í™”
    translator = JiraTicketTranslator(
        jira_url=JIRA_URL or input_base_url,
        email=JIRA_EMAIL,
        api_token=JIRA_API_TOKEN,
        openai_api_key=OPENAI_API_KEY
    )

    results_obj = translator.translate_issue(
        issue_key=issue_key,
        fields_to_translate=None  # ìë™ ê²°ì •
    )

    translation_results = results_obj.get("results", {}) if isinstance(results_obj, dict) else {}

    # ê²°ê³¼ ì¶œë ¥
    if not translation_results:
        print("âš ï¸ ë²ˆì—­ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("\nğŸ“Š Translation Results:")
        print("="*50)
        for field, content in translation_results.items():
            print(f"\n{field.upper()}:")
            print("Original:")
            print(content.get('original', ''))
            print("\nTranslated:")
            print(content.get('translated', ''))

        update_payload = translator.build_field_update_payload(translation_results)
        if not update_payload:
            print("\nâ„¹ï¸ ì—…ë°ì´íŠ¸í•  í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            confirm = input("\nJira ì´ìŠˆë¥¼ ì—…ë°ì´íŠ¸í• ê¹Œìš”? (y/n): ").strip().lower()
            if confirm == "y":
                try:
                    translator.update_issue_fields(issue_key, update_payload)
                except requests.HTTPError as exc:
                    print(f"âŒ Jira ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {exc}")
            else:
                print("â„¹ï¸ ì—…ë°ì´íŠ¸ë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")


## í•¸ë“¤ëŸ¬ í•¨ìˆ˜
def handler(event, context):
    try:
        event = event or {}
        # Parse API Gateway proxy body (JSON or x-www-form-urlencoded)
        body_raw = event.get("body") or ""
        if event.get("isBase64Encoded"):
            if isinstance(body_raw, str):
                body_raw = body_raw.encode("utf-8", "ignore")
            body_raw = base64.b64decode(body_raw).decode("utf-8", "ignore")
        headers = event.get("headers") or {}
        content_type = headers.get("content-type") or headers.get("Content-Type") or ""
        content_type = content_type.lower() if isinstance(content_type, str) else ""
        parsed = {}
        try:
            if "application/json" in content_type:
                parsed = json.loads(body_raw or "{}")
            elif "application/x-www-form-urlencoded" in content_type:
                parsed = {
                    k: (v[0] if isinstance(v, list) else v)
                    for k, v in urllib.parse.parse_qs(body_raw).items()
                }
        except Exception:
            parsed = {}
        if isinstance(parsed, dict):
            event.update(parsed)

        issue_key = event.get("issue_key")
        issue_url = event.get("issue_url")
        fields = event.get("fields_to_translate")  # Noneì´ë©´ ìë™ ê²°ì •
        do_update = event.get("update", False)
        # [ë³´ì•ˆ ìˆ˜ì •] jira_url ì˜¤ë²„ë¼ì´ë“œ ì œê±°
        # jira_url_override = event.get("jira_url")  <-- ì‚­ì œ ê¶Œì¥
        
        # [ìˆ˜ì •] í™˜ê²½ ë³€ìˆ˜ì—ì„œë§Œ URL ë¡œë“œ
        JIRA_URL = os.getenv("JIRA_URL", "https://cloud.jira.krafton.com").rstrip("/")
        JIRA_EMAIL = os.getenv("JIRA_EMAIL")
        JIRA_API_TOKEN = os.getenv("JIRA_API_TOKEN")
        OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
        
        # ... (ê²€ì¦ ë° translator ì´ˆê¸°í™” ë¡œì§ ìœ ì§€) ...
        
        if not all([JIRA_URL, JIRA_EMAIL, JIRA_API_TOKEN, OPENAI_API_KEY]):
            raise EnvironmentError("JIRA_URL, JIRA_EMAIL, JIRA_API_TOKEN, OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # issue_key ìš°ì„ , ì—†ìœ¼ë©´ URLì—ì„œ ì¶”ì¶œ
        if not issue_key:
            if issue_url:
                _, issue_key = parse_issue_url(issue_url)
            else:
                raise ValueError("issue_key ë˜ëŠ” issue_url ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")

        translator = JiraTicketTranslator(
            jira_url=JIRA_URL,
            email=JIRA_EMAIL,
            api_token=JIRA_API_TOKEN,
            openai_api_key=OPENAI_API_KEY
        )

        results_obj = translator.translate_issue(
            issue_key=issue_key,
            fields_to_translate=fields,
            perform_update=do_update
        )

        response_data = {
            "issue_key": issue_key,
            **results_obj
        }

        # [ìˆ˜ì •] API Gateway Proxy Response Format ì¤€ìˆ˜
        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps(response_data, ensure_ascii=False)
        }

    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì •í˜•í™”ëœ JSON ì‘ë‹µ ë°˜í™˜
        return {
            "statusCode": 500, # ìƒí™©ì— ë”°ë¼ 400 ë“± ë¶„ê¸° ê°€ëŠ¥
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps({
                "error": str(e),
                "type": type(e).__name__
            }, ensure_ascii=False)
        }
