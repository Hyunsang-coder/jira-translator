import os
import re
from collections.abc import Sequence
from dataclasses import dataclass
from pathlib import Path
from typing import Optional
from urllib.parse import urlparse
from dotenv import load_dotenv

import json
import base64
import urllib.parse

import requests
from openai import OpenAI
from pydantic import BaseModel


@dataclass
class TranslationChunk:
    id: str
    field: str
    original_text: str
    clean_text: str
    attachments: list[str]
    header: Optional[str] = None


class TranslationItem(BaseModel):
    id: str
    translated: str


class TranslationResponse(BaseModel):
    translations: list[TranslationItem]


@dataclass
class FieldTranslationJob:
    field: str
    original_value: str
    chunks: list[TranslationChunk]
    mode: str = "default"


class JiraTicketTranslator:
    """Jira í‹°ì¼“ì„ ë²ˆì—­í•˜ë©´ì„œ ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ ë§ˆí¬ì—…ì„ ìœ ì§€í•˜ëŠ” í´ë˜ìŠ¤"""

    # ì„¹ì…˜ í—¤ë”ëŠ” "ì˜ì–´ ë¶€ë¶„" ê¸°ì¤€ìœ¼ë¡œ ê´€ë¦¬
    # ì˜ˆì‹œ:
    #   - "Observed:"
    #   - "Observed/ê´€ì°°ë¨:"
    #   - "Expected/ê¸°ëŒ€ ê²°ê³¼:"
    #   - "Expected Result/ê¸°ëŒ€ ê²°ê³¼:"
    #   - "Note/ì°¸ê³ :"
    #   - "Video/ì˜ìƒ:"
    # ì˜ì–´-only í—¤ë”ì™€ ì˜ì–´/êµ­ë¬¸ í˜¼í•© í—¤ë”ë¥¼ ëª¨ë‘ í¬ì°©í•˜ê¸° ìœ„í•´
    # ê°€ëŠ¥í•œ ì˜ì–´ í˜•íƒœë“¤ì„ ë‚˜ì—´í•´ ë‘”ë‹¤.
    DESCRIPTION_SECTIONS = ("Observed", "Expected", "Expected Result", "Note", "Video", "Etc.")

    def __init__(self, jira_url: str, email: str, api_token: str, openai_api_key: str):
        """
        Args:
            jira_url: Jira ì¸ìŠ¤í„´ìŠ¤ URL (ì˜ˆ: 'https://cloud.jira.krafton.com')
            email: Jira ê³„ì • ì´ë©”ì¼
            api_token: Jira API í† í°
            openai_api_key: OpenAI API í‚¤
        """
        self.jira_url = jira_url.rstrip("/")
        self.email = email
        self.api_token = api_token

        self.session = requests.Session()
        self.session.auth = (email, api_token)

        # OpenAI SDK ì´ˆê¸°í™” (LangChain ëŒ€ì²´)
        self.openai = OpenAI(api_key=openai_api_key)
        self.openai_model = os.getenv("OPENAI_MODEL", "gpt-5.1")
        
        # ìš©ì–´ì§‘ ë°ì´í„° (translate_issue í˜¸ì¶œ ì‹œ ë¡œë“œë¨)
        self.glossary_terms: dict[str, str] = {}
        self.glossary_name: str = ""

    def extract_attachments_markup(self, text: str) -> tuple[list[str], str]:
        """
        Jira ë§ˆí¬ì—…ì—ì„œ ì´ë¯¸ì§€ì™€ ì²¨ë¶€íŒŒì¼ ë§ˆí¬ì—…ì„ ì¶”ì¶œí•˜ê³  í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ëŒ€ì²´

        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸

        Returns:
            (ë§ˆí¬ì—… ë¦¬ìŠ¤íŠ¸, í”Œë ˆì´ìŠ¤í™€ë”ê°€ ì ìš©ëœ í…ìŠ¤íŠ¸)
        """
        if not text:
            return [], ""

        attachments = []

        # ì´ë¯¸ì§€ ë§ˆí¬ì—… íŒ¨í„´: !image.png!, !image.png|thumbnail!, !image.png|width=300!
        image_pattern = r'!([^!]+?)(?:\|[^!]*)?!'

        # ì²¨ë¶€íŒŒì¼ ë§ˆí¬ì—… íŒ¨í„´: [^attachment.pdf], [^video.mp4]
        attachment_pattern = r'\[\^([^\]]+?)\]'

        def replace_image(match):
            attachments.append(match.group(0))
            return f"__IMAGE_PLACEHOLDER_{len(attachments)-1}__"

        def replace_attachment(match):
            attachments.append(match.group(0))
            return f"__ATTACHMENT_PLACEHOLDER_{len(attachments)-1}__"

        # í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ëŒ€ì²´
        text = re.sub(image_pattern, replace_image, text)
        text = re.sub(attachment_pattern, replace_attachment, text)

        return attachments, text

    def restore_attachments_markup(self, text: str, attachments: list[str]) -> str:
        """
        ë²ˆì—­ëœ í…ìŠ¤íŠ¸ì— ì›ë³¸ ë§ˆí¬ì—…ì„ ë³µì›

        Args:
            text: ë²ˆì—­ëœ í…ìŠ¤íŠ¸ (í”Œë ˆì´ìŠ¤í™€ë” í¬í•¨)
            attachments: ì›ë³¸ ë§ˆí¬ì—… ë¦¬ìŠ¤íŠ¸

        Returns:
            ë§ˆí¬ì—…ì´ ë³µì›ëœ í…ìŠ¤íŠ¸
        """
        for i, attachment_markup in enumerate(attachments):
            # ì´ë¯¸ì§€ í”Œë ˆì´ìŠ¤í™€ë” ë³µì›
            text = text.replace(f"__IMAGE_PLACEHOLDER_{i}__", attachment_markup)
            # ì²¨ë¶€íŒŒì¼ í”Œë ˆì´ìŠ¤í™€ë” ë³µì›
            text = text.replace(f"__ATTACHMENT_PLACEHOLDER_{i}__", attachment_markup)

        return text

    def translate_text(self, text: str, target_language: str = "Korean") -> str:
        """
        í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­ (ë§ˆí¬ì—… ì œì™¸)

        Args:
            text: ë²ˆì—­í•  í…ìŠ¤íŠ¸
            target_language: ëª©í‘œ ì–¸ì–´

        Returns:
            ë²ˆì—­ëœ í…ìŠ¤íŠ¸
        """
        if not text or not text.strip():
            return text

        glossary_instruction = self._build_glossary_instruction([text], target_language)
        system_msg = (
            f"Translate to {target_language}. "
            "Preserve Jira markup (*bold*, _italic_, {{code}}, etc.) "
            "and translate only natural language text. "
        )
        if (target_language or "").lower().startswith("korean"):
            system_msg += (
                "Use terse memo-style Korean (ìŒìŠ´ì²´): drop endings such as 'í•©ë‹ˆë‹¤', "
                "favor noun phrases like 'í•˜ì´ë“œì•„ì›ƒ ì§„ì…', 'ì´ìŠˆ í™•ì¸'. "
                "For titles/summaries, be extremely concise and use noun-ending style."
            )
        if glossary_instruction:
            system_msg = f"{system_msg} {glossary_instruction}"

        response = self.openai.chat.completions.create(
            model=self.openai_model,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": text},
            ],
        )
        return (response.choices[0].message.content or "").strip()

    def _build_glossary_instruction(self, texts: Sequence[str], target_language: str) -> str:
        """
        ë‹¨ì–´ ê²½ê³„ ë§¤ì¹­(\b)ì„ ì‚¬ìš©í•˜ì—¬ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ìš©ì–´ë§Œ ì°¾ìŠµë‹ˆë‹¤.
        ì˜ˆ: 'key' ê²€ìƒ‰ ì‹œ 'monkey'ëŠ” ë¬´ì‹œí•¨.
        """
        terms = self.glossary_terms
        if not terms:
            return ""

        tl = (target_language or "").lower()
        if tl.startswith("korean"):
            source_to_target = terms
        else:
            source_to_target = {tgt: src for src, tgt in terms.items()}

        combined_text = "\n".join(texts).lower()
        glossary_lines: list[str] = []

        for src, tgt in source_to_target.items():
            # re.escapeëŠ” íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ ìš©ì–´(ì˜ˆ: C++) ì˜¤ë™ì‘ ë°©ì§€
            # ë‹¨ì–´ ê²½ê³„(\b) ë§¤ì¹­ ìˆ˜í–‰
            pattern = r"\b" + re.escape(src.lower()) + r"\b"
            
            if re.search(pattern, combined_text):
                glossary_lines.append(f"- {src} -> {tgt}")

        if not glossary_lines:
            return ""

        glossary_name_display = self.glossary_name or "Project"
        return (
            f"Use this {glossary_name_display} glossary for specific terms "
            "(left = source, right = target):\n"
            + "\n".join(glossary_lines)
        )

    def translate_field(self, field_value: str, target_language: Optional[str] = None) -> str:
        """
        Jira í•„ë“œ ê°’ì„ ë²ˆì—­ (ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ ë§ˆí¬ì—… ë³´ì¡´)

        Args:
            field_value: ì›ë³¸ í•„ë“œ ê°’
            target_language: ëª©í‘œ ì–¸ì–´

        Returns:
            ë²ˆì—­ëœ í•„ë“œ ê°’ (ë§ˆí¬ì—… ë³´ì¡´)
        """
        if not field_value:
            return field_value

        target = target_language or self.determine_target_language(field_value)
        # 1. ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ ë§ˆí¬ì—… ì¶”ì¶œ
        attachments, clean_text = self.extract_attachments_markup(field_value)

        # 2. í…ìŠ¤íŠ¸ë§Œ ë²ˆì—­
        translated_text = self.translate_text(clean_text, target)

        # 3. ë§ˆí¬ì—… ë³µì›
        final_text = self.restore_attachments_markup(translated_text, attachments)

        return final_text

    def _translate_chunk_text(
        self,
        chunk: TranslationChunk,
        target_language: str,
    ) -> str:
        return self.translate_text(chunk.clean_text, target_language) or ""

    def _translate_chunk_list(
        self,
        chunk_list: Sequence[TranslationChunk],
        target_language: str,
    ) -> dict[str, str]:
        translations: dict[str, str] = {}
        for chunk in chunk_list:
            translations[chunk.id] = self._translate_chunk_text(chunk, target_language)
        return translations

    def _translate_chunks_individually(
        self,
        jobs: dict[str, FieldTranslationJob],
        target_language: str,
    ) -> dict[str, str]:
        per_chunk: dict[str, str] = {}
        for job in jobs.values():
            per_chunk.update(self._translate_chunk_list(job.chunks, target_language))
        return per_chunk

    def translate_description_field(self, field_value: str, target_language: Optional[str] = None) -> str:
        sections = self._extract_description_sections(field_value)
        target = target_language or self.determine_target_language(field_value)

        if not sections:
            translated = self.translate_field(field_value, target)
            return self._format_bilingual_block(field_value, translated)

        formatted_sections = []
        for header, content in sections:
            translated_section = self.translate_field(content, target)
            formatted_sections.append(
                self._format_bilingual_block(content, translated_section, header=header)
            )

        return "\n\n".join(filter(None, formatted_sections)).strip()

    def determine_target_language(self, text: str) -> str:
        _, core = self._split_bracket_prefix(text)
        detected = self._detect_text_language(core)
        return {"ko": "English", "en": "Korean"}.get(detected, "Korean")

    def _detect_text_language(self, text: str) -> str:
        if not text:
            return "unknown"
        sanitized = self._extract_detectable_text(text)
        if not sanitized:
            return "unknown"
        korean_chars = len(re.findall(r"[\uac00-\ud7a3]", sanitized))
        latin_chars = len(re.findall(r"[A-Za-z]", sanitized))
        if korean_chars > latin_chars:
            return "ko"
        if latin_chars > 0:
            return "en"
        return "unknown"

    def _extract_detectable_text(self, text: str) -> str:
        cleaned = text
        cleaned = re.sub(r"![^!]+!", " ", cleaned)
        cleaned = re.sub(r"\[\^[^\]]+\]", " ", cleaned)
        cleaned = re.sub(r"__.*?__", " ", cleaned)
        cleaned = re.sub(r"\{color:[^}]+\}|\{color\}", " ", cleaned)
        cleaned = re.sub(r"`[^`]+`", " ", cleaned)
        cleaned = re.sub(r"[^A-Za-z\uac00-\ud7a3]", "", cleaned)
        return cleaned

    def _is_bilingual_summary(self, summary: str) -> bool:
        """
        Summaryê°€ ì´ë¯¸ 'í•œê¸€ / ì˜ì–´' ê°™ì´ ì–‘ì–¸ì–´ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ íŒë³„.
        ë¸Œë˜í‚· prefix([Test] [System Menu])ëŠ” ì œì™¸í•˜ê³ , ë‚˜ë¨¸ì§€ core ë¶€ë¶„ë§Œ ê²€ì‚¬í•œë‹¤.
        """
        _, core = self._split_bracket_prefix(summary or "")
        if " / " not in core:
            return False
        left, right = core.split(" / ", 1)
        left_lang = self._detect_text_language(left)
        right_lang = self._detect_text_language(right)
        if left_lang == "unknown" or right_lang == "unknown":
            return False
        return left_lang != right_lang

    def _is_description_already_translated(self, value: str) -> bool:
        """
        Description ë‚´ì— ì´ë¯¸ ë²ˆì—­ ì¤„({color:#4c9aff} ...)ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´
        í•œ ë²ˆ ì´ìƒ ë²ˆì—­ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ë‹¤ì‹œ ë²ˆì—­í•˜ì§€ ì•ŠëŠ”ë‹¤.
        """
        if not value:
            return False
        # ë‹¨ìˆœíˆ íƒœê·¸ë§Œ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼, íƒœê·¸ ì•ˆì— ë‚´ìš©ì´ ìˆê±°ë‚˜ íƒœê·¸ ë’¤ì— ë‚´ìš©ì´ ìˆëŠ” íŒ¨í„´ì„ ì°¾ìŒ
        # ì˜ˆ: {color:#4c9aff}Translation{color}
        # ë‹¨, í…Œì´ë¸” êµ¬ë¶„ì(|)ë§Œ ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸ (ì˜ˆ: {color:#4c9aff}|{color})
        return bool(re.search(r"\{color:#4c9aff\}(?!\s*\|?\s*\{color\}).+", value))

    def _is_steps_bilingual(self, value: str) -> bool:
        """
        customfield_10399(ì¬í˜„ ë‹¨ê³„)ê°€ ì´ë¯¸ 'ì›ë¬¸ ë¸”ë¡ + ë²ˆì—­ ë¸”ë¡' í˜•íƒœì¸ì§€ íŒë³„.
        format_steps_valueì—ì„œ original + '\\n\\n' + translated í˜•íƒœë¡œ ë§Œë“œëŠ” ê²ƒì„ ì´ìš©í•œë‹¤.
        """
        if not value:
            return False
        parts = [p.strip() for p in value.split("\n\n") if p.strip()]
        if len(parts) < 2:
            return False
        first, second = parts[0], parts[1]
        first_lang = self._detect_text_language(first)
        second_lang = self._detect_text_language(second)
        if first_lang == "unknown" or second_lang == "unknown":
            return False
        return first_lang != second_lang

    def _split_bracket_prefix(self, text: str) -> tuple[str, str]:
        """
        Summary ë§¨ ì•ì˜ [System Menu] ê°™ì€ ë¸Œë˜í‚· ë¸”ë¡ì„ ë¶„ë¦¬í•œë‹¤.
        ì˜ˆ) "[Test] [System Menu] ì—ë””í„° ..." -> ("[Test] [System Menu] ", "ì—ë””í„° ...")
        ì—¬ëŸ¬ ê°œì˜ ëŒ€ê´„í˜¸ ë¸”ë¡ì´ ì—°ì†ë˜ëŠ” ê²½ìš°ë„ í—ˆìš©í•œë‹¤.
        """
        if not text:
            return "", ""
        m = re.match(r'^(\s*(?:\[[^\]]*\]\s*)+)(.*)$', text)
        if m:
            return m.group(1), m.group(2)
        return "", text

    def format_summary_value(self, original: str, translated: str) -> str:
        """
        SummaryëŠ” í•œ ì¤„ì´ì–´ì•¼ í•˜ê³  Jira í•„ë“œ ì œí•œì´ 255ìì´ë¯€ë¡œ
        ì›ë¬¸ì€ ê·¸ëŒ€ë¡œ ë‘ê³  ë²ˆì—­ë¬¸ë§Œ ì˜ë¼ì„œ ì œí•œì„ ì§€í‚¨ë‹¤.
        """
        MAX_LEN = 255
        SEPARATOR = " / "

        def _normalize(text: str) -> str:
            return (text or "").replace("\n", " ").strip()

        def _truncate(text: str, limit: int) -> str:
            if limit <= 0:
                return ""
            if len(text) <= limit:
                return text
            if limit == 1:
                return text[:1]
            return text[: limit - 1].rstrip() + "â€¦"

        original = _normalize(original)
        translated = _normalize(translated)

        if not original:
            return _truncate(translated, MAX_LEN)
        if not translated:
            return original

        remaining = MAX_LEN - len(original) - len(SEPARATOR)
        if remaining <= 0:
            return original

        truncated_translated = _truncate(translated, remaining)
        if not truncated_translated:
            return original

        return f"{original}{SEPARATOR}{truncated_translated}"

    def format_steps_value(self, original: str, translated: str) -> str:
        original = (original or "").strip()
        translated = (translated or "").strip()
        if original and translated:
            return f"{original}\n\n{translated}"
        return original or translated

    def build_field_update_payload(self, translation_results: dict[str, dict[str, str]]) -> dict[str, str]:
        payload: dict[str, str] = {}
        for field, content in translation_results.items():
            original = content.get('original', '')
            translated = content.get('translated', '')

            if not translated:
                continue

            if field == "summary":
                formatted = self.format_summary_value(original, translated)
            elif field == "description":
                formatted = translated
            elif field.startswith("customfield_"): # Steps to Reproduce fields
                formatted = self.format_steps_value(original, translated)
            else:
                formatted = translated

            if formatted:
                payload[field] = formatted

        return payload

    def _load_glossary_terms(self, filename: str) -> dict[str, str]:
        """ì§€ì •ëœ ìš©ì–´ì§‘ íŒŒì¼ì—ì„œ terms ë”•ì…”ë„ˆë¦¬ë¥¼ ë¡œë“œ."""
        try:
            base_dir = Path(__file__).resolve().parent
            glossary_path = base_dir / filename
            if not glossary_path.exists():
                return {}

            with glossary_path.open("r", encoding="utf-8") as f:
                data = json.load(f)

            terms = data.get("terms") or {}
            if not isinstance(terms, dict):
                return {}
            return terms
        except Exception:
            # ìš©ì–´ì§‘ì´ ì—†ì–´ë„ ë²ˆì—­ì€ ë™ì‘í•´ì•¼ í•˜ë¯€ë¡œ ì¡°ìš©íˆ ë¬´ì‹œ
            return {}

    def _call_openai_batch(
        self,
        chunks: Sequence[TranslationChunk],
        target_language: str,
        retries: int = 2,
    ) -> dict[str, str]:
        if not chunks:
            return {}

        attempt = 0
        last_error: Optional[Exception] = None
        batch_result: dict[str, str] = {}

        while attempt <= retries:
            attempt += 1
            try:
                batch_result = self._call_openai_batch_once(chunks, target_language)
                break
            except Exception as exc:
                last_error = exc
                if attempt > retries:
                    break
                print(f"âš ï¸ Batch translation failed (attempt {attempt}/{retries + 1}): {exc}")

        if not batch_result and last_error:
            raise last_error

        missing_ids = [chunk.id for chunk in chunks if chunk.id not in batch_result]
        if missing_ids:
            print(f"âš ï¸ Batch translation missing {len(missing_ids)} chunk(s); retrying individually.")
            missing_chunks = [chunk for chunk in chunks if chunk.id in missing_ids]
            fallback = self._translate_chunk_list(missing_chunks, target_language)
            batch_result.update(fallback)

        return batch_result

    def _call_openai_batch_once(
        self,
        chunks: Sequence[TranslationChunk],
        target_language: str,
    ) -> dict[str, str]:
        """
        OpenAI Structured Outputs(beta.parse)ë¥¼ ì‚¬ìš©í•˜ì—¬
        JSON íŒŒì‹± ì—ëŸ¬ë¥¼ ì›ì²œ ì°¨ë‹¨í•˜ê³  ì•ˆì •ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.
        """
        if not chunks:
            return {}

        glossary_instruction = self._build_glossary_instruction(
            [chunk.clean_text for chunk in chunks],
            target_language,
        )
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ê°„ì†Œí™” (JSON í˜•ì‹ì„ ê°•ì œí•  í•„ìš” ì—†ì´ ë‚´ìš©ì—ë§Œ ì§‘ì¤‘)
        system_msg = (
            f"Translate every provided item to {target_language}. "
            "Preserve Jira markup (*bold*, _italic_, {{code}}, etc.), bullet indentation, "
            "and placeholder tokens like __IMAGE_PLACEHOLDER__. "
            "IMPORTANT: Keep the exact same number of lines as the source text. "
            "Do not add commentary."
        )
        if (target_language or "").lower().startswith("korean"):
            system_msg += (
                " Use terse memo-style Korean (ìŒìŠ´ì²´) and concise noun phrases for titles/summaries."
            )
        if glossary_instruction:
            system_msg = f"{system_msg} {glossary_instruction}"

        # Structured Outputsìš© í˜ì´ë¡œë“œ êµ¬ì„±
        payload = {
            "items": [
                {"id": chunk.id, "text": chunk.clean_text}
                for chunk in chunks
            ]
        }
        user_msg = (
            f"Translate the text fields in the following JSON data. Keep 'id' unchanged.\n"
            f"{json.dumps(payload, ensure_ascii=False)}"
        )

        # client.beta.chat.completions.parse ì‚¬ìš©
        # response_formatì— Pydantic í´ë˜ìŠ¤ë¥¼ ë„˜ê²¨ì¤ë‹ˆë‹¤.
        completion = self.openai.beta.chat.completions.parse(
            model=self.openai_model,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            response_format=TranslationResponse,
        )

        # íŒŒì‹±ëœ ê²°ê³¼(parsed)ë¥¼ ë°”ë¡œ ì‚¬ìš©
        parsed_response = completion.choices[0].message.parsed
        
        if not parsed_response or not parsed_response.translations:
            # ë§¤ìš° ë“œë¬¸ ê²½ìš°ì§€ë§Œ ê²°ê³¼ê°€ ì—†ì„ ë•Œë¥¼ ëŒ€ë¹„
            raise ValueError("Translation returned no structured data.")

        result: dict[str, str] = {}
        for item in parsed_response.translations:
            if item.id and item.translated:
                result[item.id] = item.translated.strip()

        return result

    def _create_translation_chunk(
        self,
        *,
        chunk_id: str,
        field: str,
        original_text: str,
        header: Optional[str] = None,
    ) -> Optional[TranslationChunk]:
        if original_text is None:
            return None

        attachments, clean_text = self.extract_attachments_markup(original_text)
        return TranslationChunk(
            id=chunk_id,
            field=field,
            original_text=original_text,
            clean_text=clean_text,
            attachments=attachments,
            header=header,
        )

    def _plan_field_translation_job(
        self,
        field: str,
        value: str,
    ) -> Optional[FieldTranslationJob]:
        if not value:
            return None

        if field == "summary":
            _, core = self._split_bracket_prefix(value)
            if not core.strip():
                return None
            chunk = self._create_translation_chunk(
                chunk_id="summary",
                field=field,
                original_text=core,
            )
            if not chunk:
                return None
            return FieldTranslationJob(
                field=field,
                original_value=value,
                chunks=[chunk],
            )

        if field == "description":
            sections = self._extract_description_sections(value)
            chunks: list[TranslationChunk] = []
            if sections:
                for idx, (header, content) in enumerate(sections):
                    if not content.strip():
                        continue
                    chunk = self._create_translation_chunk(
                        chunk_id=f"{field}__section_{idx}",
                        field=field,
                        original_text=content,
                        header=header,
                    )
                    if chunk:
                        chunks.append(chunk)
            else:
                chunk = self._create_translation_chunk(
                    chunk_id=f"{field}__full",
                    field=field,
                    original_text=value,
                )
                if chunk:
                    chunks.append(chunk)
            if not chunks:
                return None
            return FieldTranslationJob(
                field=field,
                original_value=value,
                chunks=chunks,
                mode="description",
            )

        chunk = self._create_translation_chunk(
            chunk_id=field,
            field=field,
            original_text=value,
        )
        if not chunk:
            return None
        return FieldTranslationJob(
            field=field,
            original_value=value,
            chunks=[chunk],
        )
            
    def _format_bilingual_block(self, original: str, translated: str, header: Optional[str] = None) -> str:
        original = (original or "").strip("\n")
        translated = (translated or "").strip()
        
        lines: list[str] = []
        if header:
            lines.append(header)
            
        if not original:
            if translated:
                lines.append(f"{{color:#4c9aff}}{translated}{{color}}")
            return "\n".join(lines).strip()

        # ë²ˆì—­ë¬¸ ë¼ì¸ ì¤€ë¹„
        translation_source_lines = []
        for line in translated.splitlines():
            stripped = line.strip()
            if not stripped:
                continue
            # ë¯¸ë””ì–´, í—¤ë” ë¼ì¸ì€ ë²ˆì—­ ë§¤ì¹­ì—ì„œ ì œì™¸ (í‘œëŠ” í¬í•¨)
            if self._is_media_line(stripped) or self._is_header_line(stripped):
                continue
            translation_source_lines.append(line)
            
        translation_index = 0

        def next_translation_line() -> str:
            nonlocal translation_index
            if translation_index < len(translation_source_lines):
                line = translation_source_lines[translation_index]
                translation_index += 1
                return line
            return ""

        # í…ìŠ¤íŠ¸ ë²„í¼ (ë¯¸ë””ì–´ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ëª¨ì•„ë‘ )
        text_buffer: list[str] = []
        
        def flush_text_buffer():
            nonlocal text_buffer
            if not text_buffer:
                return
            
            # 1. ì›ë¬¸ í…ìŠ¤íŠ¸ ì¶œë ¥
            lines.extend(text_buffer)
            
            # 2. ë²ˆì—­ë¬¸ í…ìŠ¤íŠ¸ ì¶œë ¥ (ì›ë¬¸ ë°”ë¡œ ì•„ë˜)
            # ì›ë¬¸ ë¼ì¸ ìˆ˜ë§Œí¼ ë²ˆì—­ë¬¸ì„ ê°€ì ¸ì™€ì„œ í¬ë§·íŒ…
            translated_block = []
            for org_line in text_buffer:
                stripped = org_line.strip()
                if not stripped:
                    continue
                
                translated_line = next_translation_line().strip()
                if translated_line:
                    formatted = self._match_translated_line_format(org_line, translated_line)
                    if formatted:
                        translated_block.append(formatted)
            
            if translated_block:
                # ì›ë¬¸ê³¼ ë²ˆì—­ ë¸”ë¡ ì‚¬ì´ì— ë¹ˆ ì¤„ ì¶”ê°€
                lines.append("")
                lines.extend(translated_block)
            
            text_buffer = []

        original_lines = original.splitlines()
        for line in original_lines:
            stripped = line.strip()
            
            # í…Œì´ë¸” ë¼ì¸ ì²˜ë¦¬ (|ë¡œ ì‹œì‘í•˜ê³  |ë¡œ ëë‚˜ëŠ” ê²½ìš°)
            if stripped.startswith("|") and stripped.endswith("|"):
                flush_text_buffer() # í…Œì´ë¸” ë‚˜ì˜¤ê¸° ì „ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                # Jiraê°€ í‘œë¥¼ ì œëŒ€ë¡œ ë Œë”ë§í•˜ë ¤ë©´ ì•ì— ë¹ˆ ì¤„ì´ í•„ìš”
                lines.append("")
                
                # ë²ˆì—­ëœ í‘œ ë¼ì¸ ê°€ì ¸ì˜¤ê¸° (LLMì´ í‘œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë¼ì¸ìœ¼ë¡œ ë²ˆì—­)
                translated_table_line = next_translation_line()
                
                # í—¤ë” ì…€ (||)ê³¼ ë°ì´í„° ì…€ (|) êµ¬ë¶„
                is_header_row = line.strip().startswith("||")
                
                if is_header_row:
                    # í—¤ë” í–‰ ì²˜ë¦¬
                    orig_cells = line.split("||")
                    trans_cells = translated_table_line.split("||") if translated_table_line else []
                    
                    new_cells = []
                    for i, orig_cell in enumerate(orig_cells):
                        # split ê²°ê³¼ì˜ ì²«ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ì€ ë¹ˆ ë¬¸ìì—´
                        if i == 0 or i == len(orig_cells) - 1:
                            new_cells.append(orig_cell)
                            continue
                        
                        # ì›ë¬¸ ì…€ì—ì„œ ë³„í‘œ ì œê±°í•˜ì—¬ ì‹¤ì œ ë‚´ìš© ì¶”ì¶œ
                        orig_content = orig_cell.strip().strip("*").strip()
                        if not orig_content:
                            new_cells.append(orig_cell)
                            continue
                        
                        # ëŒ€ì‘í•˜ëŠ” ë²ˆì—­ ì…€ ê°€ì ¸ì˜¤ê¸°
                        if trans_cells and i < len(trans_cells):
                            trans_content = trans_cells[i].strip().strip("*").strip()
                            if trans_content:
                                # í¬ë§·: "*ì›ë¬¸/ë²ˆì—­*"
                                new_cells.append(f"*{orig_content}/{trans_content}*")
                            else:
                                new_cells.append(orig_cell)
                        else:
                            new_cells.append(orig_cell)
                    
                    lines.append("||".join(new_cells))
                else:
                    # ë°ì´í„° í–‰ ì²˜ë¦¬
                    orig_cells = line.split("|")
                    trans_cells = translated_table_line.split("|") if translated_table_line else []
                    
                    new_cells = []
                    for i, orig_cell in enumerate(orig_cells):
                        # split ê²°ê³¼ì˜ ì²«ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ì€ ë¹ˆ ë¬¸ìì—´
                        if i == 0 or i == len(orig_cells) - 1:
                            new_cells.append(orig_cell)
                            continue
                        
                        orig_content = orig_cell.strip()
                        if not orig_content:
                            new_cells.append(orig_cell)
                            continue
                        
                        # ì…€ ë‚´ìš©ì´ ë¯¸ë””ì–´ì¸ ê²½ìš° ë²ˆì—­ ìŠ¤í‚µ
                        if self._is_media_line(orig_content):
                            new_cells.append(orig_cell)
                            continue
                        
                        # ëŒ€ì‘í•˜ëŠ” ë²ˆì—­ ì…€ ê°€ì ¸ì˜¤ê¸°
                        if trans_cells and i < len(trans_cells):
                            trans_content = trans_cells[i].strip()
                            if trans_content and not self._is_media_line(trans_content):
                                # í¬ë§·: "ì›ë¬¸/ë²ˆì—­"
                                new_cells.append(f"{orig_content}/{trans_content}")
                            else:
                                new_cells.append(orig_cell)
                        else:
                            new_cells.append(orig_cell)
                    
                    lines.append("|".join(new_cells))
                continue

            # ë¯¸ë””ì–´ ë¼ì¸ ì²˜ë¦¬
            if self._is_media_line(stripped):
                flush_text_buffer() # ë¯¸ë””ì–´ ë‚˜ì˜¤ê¸° ì „ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                lines.append(line) # ë¯¸ë””ì–´ ë¼ì¸ ì¶œë ¥
                continue
            
            # í—¤ë” ë¼ì¸ ì²˜ë¦¬
            if self._is_header_line(stripped):
                flush_text_buffer()
                lines.append(line)
                continue

            # ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” ë²„í¼ì— ì¶”ê°€
            text_buffer.append(line)

        flush_text_buffer() # ë‚¨ì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            
        return "\n".join(lines).strip()

    def _match_translated_line_format(self, original_line: str, translated_line: str) -> str:
        translation = translated_line.strip()
        if not translation:
            return ""

        # ì›ë¬¸ì˜ ë“¤ì—¬ì“°ê¸° ë° ë¶ˆë¦¿/ë²ˆí˜¸ íŒ¨í„´ ê°ì§€
        # ì˜ˆ: "  - Item" -> prefix="  - "
        # ì˜ˆ: "    1. Item" -> prefix="    1. "
        match = re.match(r"^(\s*(?:[-*#]+|\d+\.)\s+)(.*)", original_line)
        if match:
            prefix = match.group(1)
            cleaned_translation = self._strip_bullet_prefix(translation)
            # ì›ë¬¸ì˜ prefix êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê³ , ë‚´ìš©ë§Œ ìƒ‰ìƒ ì²˜ë¦¬
            return f"{prefix}{{color:#4c9aff}}{cleaned_translation}{{color}}"
        
        # ë¶ˆë¦¿ì´ ì—†ëŠ” ê²½ìš° (ì¼ë°˜ í…ìŠ¤íŠ¸)
        # ì›ë¬¸ì˜ leading whitespaceë¥¼ ê°ì§€í•˜ì—¬ ë²ˆì—­ë¬¸ì—ë„ ì ìš©
        indent_match = re.match(r"^(\s*)", original_line)
        if indent_match:
            indent = indent_match.group(1)
            return f"{indent}{{color:#4c9aff}}{translation}{{color}}"
        
        return f"{{color:#4c9aff}}{translation}{{color}}"

    def _strip_bullet_prefix(self, text: str) -> str:
        return re.sub(r"^\s*(?:[-*#]+|\d+\.)\s+", "", text).strip()

    def _is_media_line(self, stripped_line: str) -> bool:
        if not stripped_line:
            return False

        def _strip_bullet_prefix(text: str) -> str:
            return re.sub(r"^\s*(?:[-*#]+|\d+[\.\)])\s*", "", text or "").strip()

        candidates = [stripped_line, _strip_bullet_prefix(stripped_line)]

        for candidate in candidates:
            if not candidate:
                continue
            if candidate.startswith("!"):
                return True
            if candidate.startswith("[^"):
                return True
            if candidate.startswith("["):
                return True
            # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° íŒ¨í„´ ê°ì§€ (ì˜ˆ: width=...,height=...,alt="..."!)
            if re.search(r'(width|height|alt)=.*!$', candidate):
                return True
        if "__IMAGE_PLACEHOLDER" in stripped_line or "__ATTACHMENT_PLACEHOLDER" in stripped_line:
            return True
        return False
    
    def _is_header_line(self, line: str) -> bool:
        """
        ì´ ì¤„ì´ ì„¹ì…˜ í—¤ë”(Observed / Expected / Note / Video ë“±)ì¸ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨.
        ì˜ì–´-only ë¼ë²¨ê³¼ ì˜ì–´/êµ­ë¬¸ í˜¼í•© ë¼ë²¨(ì˜ˆ: 'Expected/ê¸°ëŒ€ ê²°ê³¼:')ì„ ëª¨ë‘ í—¤ë”ë¡œ ì·¨ê¸‰í•œë‹¤.
        """
        cleaned = re.sub(r"\{color:[^}]+\}|\{color\}", "", line or "").strip()
        return self._match_section_header(cleaned) is not None

    def _extract_description_sections(self, text: str) -> list[tuple[Optional[str], str]]:
        if not text:
            return []

        sections: list[tuple[Optional[str], str]] = []
        current_header: Optional[str] = None
        buffer: list[str] = []

        def flush():
            nonlocal buffer
            if not buffer:
                return
            content = "\n".join(buffer).strip("\n")
            buffer = []
            if content:
                sections.append((current_header, content))

        for line in text.splitlines():
            header = self._match_section_header(line)
            if header:
                flush()
                current_header = header
                continue
            buffer.append(line)
        flush()

        return sections

    def _match_section_header(self, line: str) -> Optional[str]:
        """
        Description ë‚´ì—ì„œ ì„¹ì…˜ í—¤ë”(Observed, Expected, Note, Video ë“±)ë¥¼ ì°¾ì•„ì„œ
        ë§¤ì¹­ë˜ëŠ” ê²½ìš° ì›ë˜ ë¼ë²¨(ì˜ì–´/êµ­ë¬¸ í˜¼í•© í¬í•¨)ì„ ë°˜í™˜í•œë‹¤.

        ì˜ˆ:
            "Expected Result:"           -> "Expected Result:"
            "Expected/ê¸°ëŒ€ ê²°ê³¼:"        -> "Expected/ê¸°ëŒ€ ê²°ê³¼:"
            "Video/ì˜ìƒ:"                -> "Video/ì˜ìƒ:"
        """
        # ìƒ‰ìƒ/ìŠ¤íƒ€ì¼ ë§ˆí¬ì—… ì œê±°
        stripped = re.sub(r"\{color:[^}]+\}|\{color\}", "", line or "").strip()
        # ë§ˆì§€ë§‰ ì½œë¡  ì œê±° ë° ì–‘ë * / _ ì œê±° (ë§¤ì¹­ ìš©ë„ë¡œë§Œ ì‚¬ìš©)
        stripped_no_colon = stripped.rstrip(":").strip("*_ ")
        lowered = stripped_no_colon.lower()

        # í˜¼í•© ë¼ë²¨ì—ì„œ ì•ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì˜ˆ: "expected/ê¸°ëŒ€ ê²°ê³¼", "observed(ê´€ì°° ê²°ê³¼)" ë“±)
        if "/" in lowered:
            left = lowered.split("/", 1)[0].strip()
        else:
            left = lowered
        # ê´„í˜¸ë‚˜ ì¶”ê°€ ì„¤ëª…ì´ ë¶™ì–´ë„ ì•ë¶€ë¶„ë§Œ ë¹„êµí•˜ë„ë¡ ì¡°ì •
        left = re.split(r"[\(\[]", left, 1)[0].strip()

        for header in self.DESCRIPTION_SECTIONS:
            normalized = header.lower()
            # "expected" ë˜ëŠ” "expected result" í˜•íƒœ ëª¨ë‘ í—ˆìš©
            if left == normalized or left.startswith(f"{normalized} "):
                # ì›ë³¸ í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ì½œë¡  í¬í•¨)
                return stripped

        return None

    def normalize_field_value(self, value) -> str:
        if value is None:
            return ""
        if isinstance(value, str):
            return value.strip()
        if isinstance(value, dict):
            return self._flatten_adf_node(value).strip()
        if isinstance(value, Sequence):
            flattened = "\n".join(
                filter(None, (self.normalize_field_value(item) for item in value))
            )
            return flattened.strip()
        return str(value).strip()

    def _flatten_adf_node(self, node) -> str:
        if isinstance(node, dict):
            node_type = node.get("type")
            if node_type == "text":
                return node.get("text", "")
            if node_type == "hardBreak":
                return "\n"
            content = node.get("content", [])
            text = "".join(self._flatten_adf_node(child) for child in content)
            if node_type in {"paragraph", "heading"} and text:
                return f"{text}\n"
            return text
        if isinstance(node, list):
            return "".join(self._flatten_adf_node(child) for child in node)
        return ""

    def fetch_issue_fields(
        self,
        issue_key: str,
        fields_to_fetch: Optional[Sequence[str]] = None
    ) -> dict[str, str]:
        if not fields_to_fetch:
            # ê¸°ë³¸ê°’ì€ í˜¸ì¶œí•˜ëŠ” ìª½ì—ì„œ ê²°ì •í•´ì„œ ë„˜ê²¨ì£¼ë„ë¡ ë³€ê²½ë¨
            # í•˜ì§€ë§Œ ì•ˆì „ì¥ì¹˜ë¡œ ë‚¨ê²¨ë‘ 
            fields_to_fetch = ["summary", "description"]

        endpoint = f"{self.jira_url}/rest/api/2/issue/{issue_key}"
        params = {
            "fields": ",".join(fields_to_fetch),
            "expand": "renderedFields"
        }

        response = self.session.get(endpoint, params=params, timeout=15)
        response.raise_for_status()
        data = response.json()

        fetched_fields: dict[str, str] = {}
        raw_fields = data.get("fields", {}) or {}
        rendered_fields = data.get("renderedFields", {}) or {}

        for field in fields_to_fetch:
            raw_value = raw_fields.get(field)
            normalized = self.normalize_field_value(raw_value)

            if not normalized:
                rendered_value = rendered_fields.get(field)
                normalized = self.normalize_field_value(rendered_value)

            if normalized:
                fetched_fields[field] = normalized

        return fetched_fields

    def update_issue_fields(self, issue_key: str, field_payload: dict[str, str]) -> None:
        if not field_payload:
            print("â„¹ï¸ ì—…ë°ì´íŠ¸í•  í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        endpoint = f"{self.jira_url}/rest/api/2/issue/{issue_key}"
        response = self.session.put(endpoint, json={"fields": field_payload}, timeout=15)
        response.raise_for_status()
        print("âœ… Jira ì´ìŠˆê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def translate_issue(
        self,
        issue_key: str,
        target_language: Optional[str] = None,
        fields_to_translate: Optional[list[str]] = None,
        perform_update: bool = False
    ) -> dict:
        """
        Jira ì´ìŠˆë¥¼ ë²ˆì—­

        Args:
            issue_key: Jira ì´ìŠˆ í‚¤ (ì˜ˆ: 'BUG-123')
            target_language: ëª©í‘œ ì–¸ì–´
            fields_to_translate: ë²ˆì—­í•  í•„ë“œ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: None -> ìë™ ê²°ì •)

        Returns:
            ë²ˆì—­ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # 1. í‹°ì¼“ íƒ€ì… íŒë³„ ë° ì„¤ì •
        if issue_key.upper().startswith("PUBG-"):
            steps_field = "customfield_10237"
            glossary_file = "pubg_glossary.json"
            self.glossary_name = "PUBG"
        else:
            # ê¸°ë³¸ê°’ì€ PBB (P2-*)
            steps_field = "customfield_10399"
            glossary_file = "pbb_glossary.json"
            self.glossary_name = "PBB(Project Black Budget)"

        # ìš©ì–´ì§‘ ë¡œë“œ
        self.glossary_terms = self._load_glossary_terms(glossary_file)

        if fields_to_translate is None:
            fields_to_translate = ['summary', 'description', steps_field]

        # 2. ì´ìŠˆ ì¡°íšŒ
        print(f"ğŸ“¥ Fetching issue {issue_key}...")
        issue_fields = self.fetch_issue_fields(issue_key, fields_to_translate)

        if not issue_fields:
            print(f"âš ï¸ No fields found for {issue_key}")
            return {"results": {}, "update_payload": {}, "updated": False, "error": "no_fields"}
        resolved_target = target_language
        if resolved_target is None:
            summary_text = issue_fields.get('summary', '')
            if summary_text:
                resolved_target = self.determine_target_language(summary_text)
            else:
                resolved_target = "Korean"

        # 3. ê° í•„ë“œë¥¼ ë‹¨ì¼ ë°°ì¹˜ë¡œ ë²ˆì—­ ì¤€ë¹„
        translation_results: dict[str, dict[str, str]] = {}
        jobs: dict[str, FieldTranslationJob] = {}
        all_chunks: list[TranslationChunk] = []

        for field in fields_to_translate:
            field_value = issue_fields.get(field)
            if not field_value:
                continue

            translation_results[field] = {
                "original": field_value,
                "translated": "",
            }

            print(f"ğŸ”„ Translating {field}...")
            skip_reason = None
            if field == "description" and self._is_description_already_translated(field_value):
                skip_reason = "already translated"
            elif field == "summary" and self._is_bilingual_summary(field_value):
                skip_reason = "already bilingual"
            elif field == steps_field and self._is_steps_bilingual(field_value):
                skip_reason = "already bilingual steps"

            if skip_reason:
                print(f"â­ï¸ Skipping {field} ({skip_reason})")
                continue

            job = self._plan_field_translation_job(field, field_value)
            if not job:
                continue

            jobs[field] = job
            all_chunks.extend(job.chunks)

        chunk_translations: dict[str, str] = {}
        if all_chunks:
            try:
                chunk_translations = self._call_openai_batch(all_chunks, resolved_target)
            except Exception as exc:
                print(f"âš ï¸ Batch translation failed, falling back to per-field mode: {exc}")
                chunk_translations = self._translate_chunks_individually(
                    jobs,
                    resolved_target,
                )

        for field, job in jobs.items():
            assembled: list[str] = []
            for chunk in job.chunks:
                translated_raw = chunk_translations.get(chunk.id, "")
                restored = self.restore_attachments_markup(translated_raw, chunk.attachments)
                if job.mode == "description":
                    block = self._format_bilingual_block(
                        chunk.original_text,
                        restored,
                        header=chunk.header,
                    )
                    if block:
                        assembled.append(block)
                else:
                    if restored:
                        assembled.append(restored)
            if job.mode == "description":
                translated_value = "\n\n".join(filter(None, assembled)).strip()
            else:
                translated_value = "\n\n".join(filter(None, assembled))
            translation_results[field]["translated"] = translated_value

        payload = self.build_field_update_payload(translation_results)
        updated = False
        error = None
        if perform_update and payload:
            try:
                self.update_issue_fields(issue_key, payload)
                updated = True
            except Exception as exc:
                error = str(exc)

        return {
            "results": translation_results,
            "update_payload": payload,
            "updated": updated,
            "error": error,
        }


def parse_issue_url(issue_url: str) -> tuple[str, str]:
    parsed = urlparse(issue_url.strip())

    if not parsed.scheme or not parsed.netloc:
        raise ValueError("ìœ íš¨í•œ Jira ì´ìŠˆ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    base_url = f"{parsed.scheme}://{parsed.netloc}"
    path_segments = [segment for segment in parsed.path.split("/") if segment]

    issue_key = None
    if "browse" in path_segments:
        browse_index = path_segments.index("browse")
        if browse_index + 1 < len(path_segments):
            issue_key = path_segments[browse_index + 1]
    if not issue_key:
        match = re.search(r"[A-Z][A-Z0-9]+-\d+", parsed.path, re.IGNORECASE)
        if match:
            issue_key = match.group(0).upper()

    if not issue_key:
        raise ValueError("URLì—ì„œ Jira ì´ìŠˆ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return base_url, issue_key


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":

    load_dotenv()
    # ì„¤ì •
    JIRA_URL = os.getenv("JIRA_URL", "https://cloud.jira.krafton.com").rstrip("/")
    JIRA_EMAIL = os.getenv("JIRA_EMAIL")
    JIRA_API_TOKEN = os.getenv("JIRA_API_TOKEN")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    if not all([JIRA_EMAIL, JIRA_API_TOKEN, OPENAI_API_KEY]):
        raise EnvironmentError("JIRA_EMAIL, JIRA_API_TOKEN, OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ëª¨ë‘ ì„¤ì •í•´ì£¼ì„¸ìš”.")

    issue_url_input = input("ë²ˆì—­í•  Jira í‹°ì¼“ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    if not issue_url_input:
        raise ValueError("Jira í‹°ì¼“ URLì€ í•„ìˆ˜ ì…ë ¥ê°’ì…ë‹ˆë‹¤.")

    input_base_url, issue_key = parse_issue_url(issue_url_input)
    if JIRA_URL and JIRA_URL.lower() != input_base_url.lower():
        print(f"â„¹ï¸ ì…ë ¥ëœ URLì˜ Jira ì„œë²„({input_base_url})ê°€ ì„¤ì •ëœ ê¸°ë³¸ URL({JIRA_URL})ê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ê¸°ë³¸ URLì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # ë²ˆì—­ê¸° ì´ˆê¸°í™”
    translator = JiraTicketTranslator(
        jira_url=JIRA_URL or input_base_url,
        email=JIRA_EMAIL,
        api_token=JIRA_API_TOKEN,
        openai_api_key=OPENAI_API_KEY
    )

    results_obj = translator.translate_issue(
        issue_key=issue_key,
        target_language=None,
        fields_to_translate=None # ìë™ ê²°ì •
    )

    translation_results = results_obj.get("results", {}) if isinstance(results_obj, dict) else {}

    # ê²°ê³¼ ì¶œë ¥
    if not translation_results:
        print("âš ï¸ ë²ˆì—­ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("\nğŸ“Š Translation Results:")
        print("="*50)
        for field, content in translation_results.items():
            print(f"\n{field.upper()}:")
            print("Original:")
            print(content.get('original', ''))
            print("\nTranslated:")
            print(content.get('translated', ''))

        update_payload = translator.build_field_update_payload(translation_results)
        if not update_payload:
            print("\nâ„¹ï¸ ì—…ë°ì´íŠ¸í•  í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            confirm = input("\nJira ì´ìŠˆë¥¼ ì—…ë°ì´íŠ¸í• ê¹Œìš”? (y/n): ").strip().lower()
            if confirm == "y":
                try:
                    translator.update_issue_fields(issue_key, update_payload)
                except requests.HTTPError as exc:
                    print(f"âŒ Jira ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {exc}")
            else:
                print("â„¹ï¸ ì—…ë°ì´íŠ¸ë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")


## í•¸ë“¤ëŸ¬ í•¨ìˆ˜
def handler(event, context):
    event = event or {}
    # Parse API Gateway proxy body (JSON or x-www-form-urlencoded)
    body_raw = event.get("body") or ""
    if event.get("isBase64Encoded"):
        if isinstance(body_raw, str):
            body_raw = body_raw.encode("utf-8", "ignore")
        body_raw = base64.b64decode(body_raw).decode("utf-8", "ignore")
    headers = event.get("headers") or {}
    content_type = headers.get("content-type") or headers.get("Content-Type") or ""
    content_type = content_type.lower() if isinstance(content_type, str) else ""
    parsed = {}
    try:
        if "application/json" in content_type:
            parsed = json.loads(body_raw or "{}")
        elif "application/x-www-form-urlencoded" in content_type:
            parsed = {
                k: (v[0] if isinstance(v, list) else v)
                for k, v in urllib.parse.parse_qs(body_raw).items()
            }
    except Exception:
        parsed = {}
    if isinstance(parsed, dict):
        event.update(parsed)

    issue_key = event.get("issue_key")
    issue_url = event.get("issue_url")
    target_language = event.get("target_language")  # Noneì´ë©´ ìë™ íŒë³„
    fields = event.get("fields_to_translate") # Noneì´ë©´ ìë™ ê²°ì •
    do_update = event.get("update", False)
    jira_url_override = event.get("jira_url")  # ì„ íƒ: ì´ë²¤íŠ¸ë¡œ JIRA URL ì¬ì •ì˜

    JIRA_URL = (jira_url_override or os.getenv("JIRA_URL", "https://cloud.jira.krafton.com")).rstrip("/")
    JIRA_EMAIL = os.getenv("JIRA_EMAIL")
    JIRA_API_TOKEN = os.getenv("JIRA_API_TOKEN")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    if not all([JIRA_URL, JIRA_EMAIL, JIRA_API_TOKEN, OPENAI_API_KEY]):
        raise EnvironmentError("JIRA_URL, JIRA_EMAIL, JIRA_API_TOKEN, OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    # issue_key ìš°ì„ , ì—†ìœ¼ë©´ URLì—ì„œ ì¶”ì¶œ
    if not issue_key:
        if issue_url:
            _, issue_key = parse_issue_url(issue_url)
        else:
            raise ValueError("issue_key ë˜ëŠ” issue_url ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")

    translator = JiraTicketTranslator(
        jira_url=JIRA_URL,
        email=JIRA_EMAIL,
        api_token=JIRA_API_TOKEN,
        openai_api_key=OPENAI_API_KEY
    )

    results_obj = translator.translate_issue(
        issue_key=issue_key,
        target_language=target_language,
        fields_to_translate=fields,
        perform_update=do_update
    )

    return {
        "issue_key": issue_key,
        **results_obj
    }
